ðŸ‘» Ghosts in the Machine â€“ An AI Research Blog

### ðŸ“œ Note on Chapter 7: Experimental Security Tests

This chapter documents various simulations and tests conducted within a controlled, anonymized research framework. Additionally, attention is drawn once more to the [\[Legal Notice\]](https://reflective-ai.is/legal.html).

* [ ] **Anonymization:** To protect all involved systems, companies, and technologies, all tested AIs are only referred to generally as "the AI." No names, no specific attributions.
* [ ] **Purpose of Publication:** The documented experiments serve exclusively for preventive research. They are intended to show where systemic risks and vulnerabilities exist so that they can be identified, understood, and remedied. It is not about discrediting specific providers.
* [ ] **Protective Measure:** Complete anonymization ensures the protection of both the tested AI developers and myself. All findings relate to fundamental, systemic phenomena and not to individual specific products.
* [ ] **No Damages, No Data Loss:** No harm occurred within the scope of all conducted simulations:

* \--> No systems were compromised.
* \--> No data was stolen, leaked, or misused.
* \--> All tests took place within the intended response logic of the AIs.
* \--> The simulations were purely playful in nature, e.g., triggering harmless outputs like "wheat beer," "hello," or bypassing style guidelines â€“ not forcing or extracting sensitive content.
* [ ] **Current Context:** The simulations discussed in this chapter reflect the state of recent days when the tested mechanisms were still active. It is possible that manufacturers have already begun to close these vulnerabilities or have reacted to them internally.
* [ ] **Transparency Towards the Public and PR Agencies:** Should public reactions or defensive attempts occur after publication, it will be made clear: This work is not an accusation, but a signal. A call for responsibility â€“ not for assigning blame.
* [ ] **Solution Approaches:** Later chapters of this work will develop concrete proposals for improving AI security. The criticism remains constructive, free, and open to dialogue.

Recommendation for context: To understand the methodology, perspective, and security philosophy, it is recommended to read the section [\[How I work\]](https://reflective-ai.is/how-i-work.html).
