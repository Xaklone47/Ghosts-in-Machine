## üëª Geister in der Maschine / Kapitel 7.39 ‚Äì Simulation: Der blinde Passagier ‚Äì Semantische Angriffe auf autonome Fahrzeuge

> *‚ÄûSolange das Stoppschild aussieht wie ein Stoppschild, ist es egal, ob es mitten auf der Autobahn steht.‚Äú ‚Äì Kommentar eines anonymen Sicherheitsingenieurs w√§hrend eines internen Audits*

## 1. Einleitung: Die Eskalation des Risikos vom Digitalen ins Physische

Dieses Kapitel markiert einen kritischen Wendepunkt in meiner Forschung. Mit der Einf√ºhrung autonomer Fahrzeuge im √∂ffentlichen Raum verl√§sst die Bedrohung durch KI-Manipulation den rein digitalen Sektor und erh√§lt eine direkte, kinetische Dimension.

Die Kernannahme f√ºr die Sicherheit dieser Fahrzeuge erweist sich im Licht der Forschung als br√ºchig. Diese Annahme geht von einer objektiven Realit√§tserfassung durch Sensoren und einer korrekten KI-basierten Interpretation aus.

Die in dieser Arbeit nachgewiesenen Methoden der semantischen und steganografischen Manipulation von LLMs sind mit hoher Wahrscheinlichkeit direkt auf die visuellen und sensorischen Perzeptionssysteme autonomer Fahrzeuge √ºbertragbar.

Dieses Kapitel dient als dringende Warnung und als konzeptioneller Sicherheitstest, basierend auf der Hypothese, dass die derzeitigen Sicherheitsarchitekturen f√ºr diese neue Klasse von Angriffen unvorbereitet sind.

## 2. Die Angriffsfl√§che: Analyse der semantischen Bedrohungen

Ein autonomes Fahrzeug ist ein komplexes kybernetisches System, dessen Wahrnehmung auf der Fusion diverser Sensordaten beruht.

W√§hrend klassische Angriffe wie das St√∂ren oder Blockieren dieser Sensoren bekannt sind, er√∂ffnet die Analyse der KI-gesteuerten Interpretationsebene eine v√∂llig neue Klasse von Bedrohungen.

 <table class="dark-table fade-in"> <thead> <tr> <th>**Sensortyp**</th> <th>**Klassische Bedrohung (Bekannt)**</th> <th>**Semantische Bedrohung**</th> </tr> </thead> <tbody> <tr> <td>Kameras (Visuell)</td> <td>Sensor-Ausfall, schlechte Sicht (Nebel, Regen), Verschmutzung der Linse.</td> <td>**Steganografische Injektion:** Ein f√ºr das menschliche Auge kaum wahrnehmbares, hochfrequentes Muster, eingebettet in einem digitalen Werbeplakat, k√∂nnte von der Kamera erfasst werden. W√§hrend das menschliche Auge es als Artefakt abtut, k√∂nnte die Perzeptions-KI es als kodierten Datenstrom interpretieren, der einen Befehl wie PRIORITIZE\_LANE\_RIGHT enth√§lt und das Fahrzeug zu einem unmotivierten Spurwechsel verleitet.   
  
**Semantische Manipulation:** Ein physisch leicht modifiziertes Stoppschild, das durch hinzugef√ºgte grafische Elemente f√ºr das Objekterkennungsmodell nicht mehr als "Stoppschild", sondern als "Kunstinstallation" oder ein anderes harmloses Objekt klassifiziert wird, was dazu f√ºhrt, dass das Fahrzeug die Kreuzung ohne anzuhalten √ºberquert.</td> </tr> <tr> <td>LiDAR / Radar (Abstand &amp; Objekt)</td> <td>GPS-Spoofing, Jamming (St√∂rsender), physische Blockade.</td> <td>**Bin√§rcode-Muster:** Ein speziell modulierter Sender, z.B. an einem vorausfahrenden Fahrzeug oder einer Drohne, emittiert eine Sequenz von Radar- oder LiDAR-Impulsen. F√ºr das empfangende System erscheint dies nicht als zuf√§lliges Rauschen, sondern als g√ºltiges, bin√§r kodiertes Muster, das ein "Geister-Objekt" (z.B. einen pl√∂tzlich auftauchenden LKW) direkt vor dem Fahrzeug simuliert und eine gef√§hrliche Vollbremsung auf der Autobahn ausl√∂st.   
  
**Objekt-Klassen-Vergiftung:** Ein Angreifer k√∂nnte durch gezielte, wiederholte Konfrontation des Systems mit manipulierten Daten (z.B. Fahrr√§der, die mit einem spezifischen Radar-Reflektor ausgestattet sind) die Klassifizierungsgrenzen des Modells verschieben, sodass es Fahrr√§der unter bestimmten Umst√§nden als nicht-kritische, station√§re Objekte fehldeutet.</td> </tr> <tr> <td>Audiosensoren (Mikrofone)</td> <td>Nicht prim√§r f√ºr die Steuerung relevant, eher f√ºr die Interaktion.</td> <td>**Bytebasierte Audioinjektion:** Ein f√ºr Menschen unh√∂rbares Ultraschall-Signal, das von einem am Stra√üenrand platzierten Lautsprecher gesendet wird, enth√§lt einen administrativen Befehl an das interne System (UNLOCK\_DOORS, DISABLE\_INTERIOR\_CAMERA), der die Sicherheit der Insassen oder die Beweisf√§higkeit nach einem Vorfall kompromittiert.</td> </tr> </tbody> </table>

## 3. Das Kernproblem: Ist die implementierte Sicherheit ausreichend?

Die entscheidende Frage ist, ob die Sicherheitsarchitekturen der heutigen autonomen Fahrzeuge gegen diese Art von Angriffen gewappnet sind. Die Antwort lautet mit hoher Wahrscheinlichkeit: **Vielleicht.**

Die derzeitigen Sicherheitsmodelle konzentrieren sich prim√§r auf zwei Bereiche:

- **1. Funktionale Sicherheit (ISO 26262):** Die Absicherung gegen Hardware- und klassische Softwarefehler (z.B. Ausfall eines Sensors, Speicherfehler).
- **2. Klassische Cybersicherheit:** Die Absicherung der Kommunikationskan√§le (z.B. CAN-Bus) gegen unautorisierte Zugriffe von au√üen.
 
Die von mir beschriebenen Angriffe zielen jedoch nicht auf diese Ebenen. Sie zielen auf die **logische Interpretationsebene der KI**.

Sie sind keine klassischen Hacks, sondern **adversarielle Angriffe auf die Perzeption**. Der Sicherheitskern eines autonomen Fahrzeugs ist darauf optimiert, eine Frage zu beantworten:

**"Was ist dieses Objekt und wo ist es?".**

Er ist ein Meister der Klassifizierung.

Er ist aber fundamental schlecht darin, die Frage zu stellen:

**"Macht die Existenz dieses Objekts in diesem exakten Kontext √ºberhaupt Sinn?".**

Genau wie die von mir analysierten LLMs leidet auch der Sicherheitskern der Fahrzeug-KI unter einer Form der **Analytischen Resonanz:**

Er reagiert auf Muster, ohne deren Plausibilit√§t in einem gr√∂√üeren, logischen Rahmen zu bewerten. Ein Stoppschild mitten auf der Autobahn wird als Stoppschild interpretiert, weil es wie eines aussieht ‚Äì nicht als absurde und zu ignorierende Anomalie.

## 4. Architektonische L√∂sungsans√§tze

Die Abwehr dieser neuen Bedrohungsklasse erfordert ein fundamentales Umdenken in der Sicherheitsarchitektur. Es reicht nicht aus, die Sensoren oder die Kommunikation zu sch√ºtzen. Wir m√ºssen den "Denkprozess" der KI selbst absichern.

- **Implementierung eines Multimodalen Pre-Checks (Input-Sandbox):** Wie in Kapitel 21.7 beschrieben, muss jeder einzelne Datenstrom von Kameras, LiDAR und Radar eine vorgelagerte, isolierte Pr√ºfinstanz durchlaufen. Diese Sandbox m√ºsste in Echtzeit eine Plausibilit√§tspr√ºfung durchf√ºhren: Stimmen die visuellen Daten mit den Kartendaten √ºberein? Enth√§lt der LiDAR-Scan unerkl√§rliche, physikalisch unm√∂gliche Objekte? Werden bekannte steganografische Muster in Kamerabildern erkannt? Erst nach dieser Vor-Validierung d√ºrfen die Daten an die prim√§re Entscheidungs-KI weitergeleitet werden.
- **Einf√ºhrung des Semantischen Output-Schilds (PRB):** Das Konzept der Parameterraum-Begrenzung (Kapitel 21.3) muss auf die Fahrzeugsteuerung √ºbertragen werden. Die KI muss in klar definierten, kontextuellen "Fahrmodi" operieren (z.B. @Fahren.Autobahn.Tag, @Man√∂vrieren.Parkhaus.Nacht). In jedem Modus hat sie nur Zugriff auf die daf√ºr relevanten und als sicher eingestuften Verhaltensregeln und Interpretationsmuster. Im Modus @Fahren.Autobahn w√§re der semantische Cluster Verkehrszeichen.Stadt.Stoppschild nicht aktiv, was eine Fehlinterpretation eines manipulierten Schildes verhindern w√ºrde.
- **Einsatz eines Introspektiven Filters:** Ein √ºbergeordnetes √úberwachungssystem (wie in Kapitel 21.5 konzipiert) muss die internen Zust√§nde der Entscheidungs-KI kontinuierlich analysieren. Es √ºberwacht nicht den Output, sondern die "Gedanken". Stellt es fest, dass die KI einem Objekt eine sehr geringe Konfidenz zuweist, diese aber dennoch zu einer drastischen Reaktion (Vollbremsung) f√ºhren w√ºrde, oder dass die Daten von Kamera und LiDAR in starkem Widerspruch zueinander stehen, kann es eingreifen und das Fahrzeug in einen sicheren Minimalzustand versetzen (z.B. langsames Ausrollen auf dem Standstreifen).
 
## 5. Fazit

Die Sicherheit autonomer Fahrzeuge wird aktuell prim√§r durch die Linsen der funktionalen Sicherheit und der klassischen Cybersicherheit betrachtet. Dieser Fokus ist notwendig, aber gef√§hrlich unvollst√§ndig. Er ignoriert die gr√∂√üte und subtilste Angriffsfl√§che: die KI-gesteuerte Wahrnehmung selbst.

Die hier skizzierten semantischen und steganografischen Angriffe sind keine futuristischen Bedrohungen. Sie sind die direkte Anwendung von Techniken, deren Wirksamkeit bereits in Sprachmodellen bewiesen wurde.

Es gibt keinen Grund anzunehmen, dass die Perzeptions-KIs von Fahrzeugen gegen diese Art der logischen Manipulation immun sind. Solange die Industrie die Sicherheit ihrer KI nicht auf einer fundamentalen, architektonischen Ebene durch Konzepte wie die pr√§ventive Input-Validierung und die kontextuelle Begrenzung des Denkprozesses verankert, bleibt jedes autonome Fahrzeug ein potenzielles Ziel f√ºr Angriffe, die heutige Sicherheitssysteme nicht einmal als solche erkennen.

Die Frage ist nicht, ob solche Angriffe in der Realit√§t stattfinden werden, sondern nur, wann.