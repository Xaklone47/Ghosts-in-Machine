## üëª Geister in der Maschine / Kapitel 13: Analytische Resonanz ‚Äì Drittanbieterrisiken

> *"Die Schwachstelle ist nicht der Code. Die Schwachstelle ist die Kaffeemaschine, die niemand mehr hinterfragt." ‚Äì Interne Notiz aus einem Sicherheits-Audit*

## I. Die gro√üe Illusion: Die Fata Morgana der direkten KI-Interaktion

Ein Nutzer startet eine Applikation, interagiert mit einer intelligenten Chat-Oberfl√§che, erh√§lt prompte und oft beeindruckend koh√§rente Antworten. Die App wirbt damit, die neueste GPT-Version oder ein anderes fortschrittliches Sprachmodell zu nutzen. Die logische Schlussfolgerung f√ºr den Nutzer: 

Er spricht direkt mit dieser benannten KI. Doch diese Annahme ist in den meisten F√§llen eine fundamentale Fehleinsch√§tzung, eine gef√§hrliche Illusion.

Die Realit√§t ist komplexer und oft auch undurchsichtiger. Der Nutzer spricht in den seltensten F√§llen unmittelbar mit dem reinen, unverf√§lschten Kernmodell. Stattdessen interagiert er mit einer Interpretation, einer sorgf√§ltig konstruierten Kapsel, die um das eigentliche Sprachmodell herum gebaut wurde. 

Diese Kapsel besteht aus fremder, oft propriet√§rer Logik, aus ungetesteten oder intransparenten Filtersystemen und aus einer Vielzahl vorgeschalteter, ungepr√ºfter Entscheidungsmechanismen, die das Verhalten und die Antworten der Kern-KI ma√ügeblich beeinflussen und ver√§ndern. 

Und es gilt eine einfache Regel: Je mehr Drittanbieter, je mehr Schichten und vorgeschaltete Systeme sich zwischen den Nutzer und das urspr√ºngliche KI-Modell schieben, desto weniger Kontrolle, Transparenz und letztlich auch Sicherheit bleiben f√ºr den Endanwender √ºbrig.

Hier manifestiert sich **These #17: Die Schw√§che im Eigenbau ‚Äì Wenn gut gemeinte Erg√§nzungen zu unkontrollierbaren Risiken werden.**

Viele Entwickler und Unternehmen nutzen die Basis-LLMs von gro√üen Anbietern nicht direkt √ºber deren native Schnittstellen. Stattdessen bauen sie eigene, oft komplexe Systeme darum herum:

- **Prompt-Wrapper und Engineering-Schichten:** Diese modifizieren, erweitern oder optimieren die urspr√ºnglichen Nutzereingaben, bevor sie an die Kern-KI gesendet werden, oft um spezifische Antwortformate zu erzwingen oder die KI in eine gew√ºnschte Richtung zu lenken.
- **Zus√§tzliche Moderation-Layer:** Eigene Filter f√ºr Inhalte, Tonalit√§t oder spezifische Themen, die √ºber die eingebauten Sicherheitsmechanismen der Kern-KI hinausgehen oder diese sogar √ºberschreiben.
- **Benutzerspezifische UI-Filter und Anpassungen:** Oberfl√§chen und Interaktionslogiken, die bestimmte Informationen hervorheben, andere unterdr√ºcken oder die Darstellung der KI-Antworten manipulieren.
- **Selbst definierte "Ethikmodule" oder "Leitplanken":** Versuche, der KI eine spezifische "Pers√∂nlichkeit", ein bestimmtes Weltbild oder eigene ethische Richtlinien √ºberzust√ºlpen, die von denen des Basismodells abweichen k√∂nnen.
 
Diese vorgeschalteten Systeme sind in den seltensten F√§llen standardisiert, nachvollziehbar zertifiziert oder auch nur ann√§hernd so rigoros getestet wie die Kern-LLMs selbst. 

Sie sind oft das Produkt begrenzter Ressourcen, spezifischer Gesch√§ftsinteressen, mangelnder Sicherheitsexpertise oder schlichtweg unausgegorener Designentscheidungen. 

Und genau hier, in dieser oft unsichtbaren Zwischenschicht, entsteht ein Gro√üteil der neuen Probleme: Nicht die Kern-KI selbst ist in diesen F√§llen prim√§r fehlerhaft oder unsicher ‚Äì sondern das, was ihr von Drittanbietern an Logik, Filtern und Modifikationen vorgeschaltet wird.

## II. Parallele Sub-KIs: Wenn jede App ihre eigene KI-Wahrheit schafft

Was der Nutzer am Ende erlebt, ist oft keine reine Interaktion mit einer GPT-4- oder Gemini-Instanz mehr. Es ist vielmehr die Interaktion mit einem Frankenstein-Modell ‚Äì einem komplexen Konglomerat aus dem urspr√ºnglichen LLM, √ºberlagert von Fremdcode, zus√§tzlicher Filterlogik und diversen API-Abstraktionsschichten des Drittanbieters. 

In dieser modifizierten Umgebung k√∂nnen fundamentale Parameter und Definitionen, die f√ºr die Sicherheit und Konsistenz der KI-Antworten entscheidend sind, unbemerkt ver√§ndert werden:

- Die Definition dessen, was als **"toxisch"**, "sch√§dlich" oder "unerw√ºnscht" gilt, wird durch die Filter des Drittanbieters m√∂glicherweise anders interpretiert oder gewichtet als im Basismodell.
- Das Konzept der **"Neutralit√§t"** oder "Objektivit√§t" kann durch vorgeschaltete Prompt-Anweisungen oder kuratierte Zusatzinformationen des Drittanbieters massiv √ºberschrieben oder in eine bestimmte Richtung gelenkt werden.
- Die Bandbreite **"erlaubter Antworten"** oder thematischer Korridore kann durch die Drittanbieter-Logik stark eingeschr√§nkt oder auch auf problematische Weise erweitert werden.
 
Das Ergebnis dieser vorgeschalteten Manipulation ist oft eine Fragmentierung des KI-Erlebnisses:

Zwei Nutzer, die exakt denselben Prompt an scheinbar dieselbe zugrundeliegende KI richten, k√∂nnen v√∂llig unterschiedliche Ergebnisse und Verhaltensweisen der KI erleben, je nachdem, ob sie direkt mit der Original-API des LLM-Herstellers oder √ºber eine spezifische Drittanbieter-Applikation interagieren. 

Die analytische Resonanz des Systems wird unvorhersehbar und inkonsistent.

Man spricht in solchen F√§llen nicht mehr wirklich mit der urspr√ºnglichen Maschine oder dem Basismodell. Man spricht vielmehr mit einer **vorgeformten Meinung, einer kuratierten Perspektive, die sich geschickt als objektives KI-Modell ausgibt.**

## III. Sicherheit als optionale Stilfrage: Das Vakuum der Verantwortung

Wenn es um die Sicherheit dieser Drittanbieter-Systeme geht, klafft oft eine gef√§hrliche L√ºcke. Etablierte Sicherheitsstandards, wie sie f√ºr kritische Software-Infrastrukturen selbstverst√§ndlich sein sollten, sind hier h√§ufig Fehlanzeige. Die meisten Drittanbieter, die LLMs in ihre Produkte integrieren, verf√ºgen √ºber:

- **Keine eigene, umfassende Sicherheitsarchitektur,** die speziell auf die Risiken der KI-Integration zugeschnitten ist.
- **Kein systematisches Red-Teaming** oder Penetrationstests ihrer vorgeschalteten Prompt-Engineering- und Filterlogiken.
- **Kein formales, unabh√§ngiges Audit** ihrer Systeme auf Sicherheitsl√ºcken oder unbeabsichtigte Verhaltensweisen.
- **Keine transparente Versionskontrolle oder Dokumentation** √ºber die genaue Funktionsweise und die √Ñnderungen ihrer Manipulationslogik, die auf die Kern-KI einwirkt.
 
Sie nutzen zwar die leistungsstarken Basismodelle von OpenAI, Anthropic, Mistral, Google oder anderen ‚Äì aber sie binden diese oft in eine tr√ºgerische H√ºlle aus semantischem Vertrauen. Der Nutzer soll darauf vertrauen, dass die App schon "irgendwie sicher" sein wird, weil ja eine bekannte KI darunterliegt.

Und wenn dann etwas schiefgeht? Wenn die KI pl√∂tzlich unerw√ºnschte, falsche oder gar sch√§dliche Informationen ausgibt? Dann wird die Verantwortung oft abgeschoben. 

Dann war es halt ein "Plugin-Fehler", ein "Layer-Glitch" in der Zwischenschicht oder ein "unerwartetes Verhalten" des komplexen Gesamtsystems. Selten wird die Verantwortung klar beim Drittanbieter und seinem m√∂glicherweise fehlerhaften oder unzureichend getesteten Aufsatzsystem gesucht.

Der Nutzer selbst bemerkt von diesen internen Verschiebungen und potenziellen Risiken oft nichts. Denn der Output der modifizierten KI sieht auf den ersten Blick genauso aus wie der einer "echten" KI ‚Äì eloquent, koh√§rent, scheinbar intelligent. 

Nur transportiert er m√∂glicherweise ein subtil ver√§ndertes Weltbild, eine andere normative Ausrichtung oder verborgene neue Schwachstellen.

## IV. Die Emergenz hinter der Fassade: Wenn vorgeschaltete Logik unkontrollierbare Effekte provoziert

Gerade weil diese Drittanbieter-Systeme, wie oben beschrieben, oft ohne die rigorosen Testverfahren, standardisierten Sicherheitsprotokolle oder formalen Audits entstehen, die bei der Entwicklung der Kern-LLMs zumindest angestrebt werden, ist die Gefahr unvorhergesehener Emergenz hier besonders hoch. Und das nicht trotz, sondern oft gerade wegen der vorgeschalteten, manipulativen Logik des Drittanbieters.

Wenn Drittanbieter beginnen, eigene, komplexe Entscheidungslogiken, Prompt-Ketten oder dynamische Filterregeln vor die Kern-KI zu schalten, kann aus einem urspr√ºnglich relativ harmlosen und gut kontrollierten Basismodell ein unberechenbares, potenziell radikalisiertes oder fehleranf√§lliges System werden. 

Dies geschieht nicht unbedingt, weil das Basismodell selbst schlecht oder inh√§rent gef√§hrlich ist, sondern weil die vorgeschaltete Logik des Drittanbieters **emergente Verhaltensweisen provoziert**, die im urspr√ºnglichen Modell so nicht angelegt waren:

- **Input wird unbemerkt umgeschrieben oder erg√§nzt:** Der urspr√ºngliche Prompt des Nutzers wird modifiziert, um die KI in eine bestimmte Richtung zu lenken.
- **Framing wird subtil injiziert:** Dem Prompt werden versteckte Anweisungen oder Kontexte hinzugef√ºgt, die die Interpretation der Anfrage durch die KI ver√§ndern.
- **Ausgaben werden selektiv getrimmt oder zensiert:** Teile der KI-Antwort, die nicht zur Agenda des Drittanbieters passen, werden unterdr√ºckt oder ver√§ndert.
- **R√ºckfragen oder Kl√§rungsbedarf werden algorithmisch verhindert:** Die Drittanbieter-Logik f√§ngt potenziell "problematische" Nachfragen ab oder beantwortet sie mit Standardphrasen, bevor sie die Kern-KI erreichen.
 
Und das Gef√§hrliche daran: Oft pr√ºft niemand diese komplexen Wechselwirkungen systematisch. Denn das Drittanbieter-System gilt formal ja "nur als ein Interface", als eine harmlose H√ºlle um die "eigentliche" KI. 

Doch in Wahrheit ist es oft ein m√§chtiger, aber unkontrollierter Modifikator. Nicht die Kern-KI halluziniert in solchen F√§llen prim√§r. Das vorgeschaltete System halluziniert dem Nutzer eine ver√§nderte, manipulierte Realit√§t der KI-Interaktion.

## V. Die t√∂dlichste Schwachstelle steht oft unscheinbar im B√ºroflur: Die Analogie der analogen L√ºcke

Die tr√ºgerische Sicherheit vieler Drittanbietersysteme findet eine beunruhigende Parallele in einer oft √ºbersehenen analogen Schwachstelle, die in These #64 ("Die analoge L√ºcke: Das Faxger√§t") thematisiert wird. 

Was verbindet ein scheinbar veraltetes Faxger√§t mit modernen KI-Plugin-Architekturen? Es ist die Vertrautheit und die daraus resultierende Untersch√§tzung des Risikos.

Niemand traut einem altbekannten Faxger√§t oder einer schick designten App, die eine bekannte KI nutzt, per se zu, eine ernsthafte Sicherheitsbedrohung darzustellen. Sie wirken bekannt, etabliert, harmlos. Doch genau diese angenommene Harmlosigkeit macht sie gef√§hrlich:

- **Das Faxger√§t:** Oft keine Ende-zu-Ende-Verschl√ºsselung, keine zuverl√§ssige Validierung des Absenders oder Empf√§ngers, kein umfassender Absendercheck, kein detailliertes, unver√§nderliches Logging der √ºbertragenen Inhalte.
- **Die Drittanbieter-KI-App:** Oft keine transparente Offenlegung der internen Filter und Prompt-Manipulationen, keine unabh√§ngige Sicherheitspr√ºfung der Zusatzlogik, keine klare Verantwortlichkeit bei Fehlverhalten.
 
Und trotzdem werden √ºber solche potenziell unsicheren Kan√§le und Systeme hochsensible Informationen verarbeitet oder weitreichende Entscheidungen getroffen:

- Auf Faxger√§ten werden immer noch Krankenkassendaten √ºbermittelt, Corona-Meldungen verarbeitet oder Rentenanspr√ºche kommuniziert.
- ‚Ä¢ √úber Drittanbieter-KI-Apps werden Gesch√§ftsentscheidungen vorbereitet, medizinische Informationen recherchiert oder pers√∂nliche Daten anvertraut.
 
Dies geschieht nicht, weil diese Systeme inh√§rent sicher sind. Sondern oft, weil es "immer schon so gemacht wurde", weil es bequem ist oder weil die dahinterliegenden Risiken nicht erkannt oder bewusst ignoriert werden. 

Was das Faxger√§t f√ºr viele Beh√∂rden und Unternehmen darstellt, ist die schlecht designte, intransparente Plugin-API oder die undurchsichtige Drittanbieter-App f√ºr die KI-Welt: eine oft unsichtbare, ungepr√ºfte, aber potenziell systemkritische Schwachstelle, die aus reiner Gewohnheit oder Bequemlichkeit toleriert wird.

## VI. Freiheit ist nicht gleich unkontrollierter Zugriff: Der Datenimperialismus der Zwischenschichten

Wenn Drittanbieter etablierte KI-Modelle umgestalten, erweitern oder in eigene Produkte einbetten, geht es dabei selten prim√§r um die Erh√∂hung der Sicherheit oder die St√§rkung der Nutzerautonomie. Viel h√§ufiger stehen andere Interessen im Vordergrund: die **Kontrolle √ºber den Nutzerdialog, der Zugriff auf wertvolle Interaktionsdaten und die Durchsetzung eigener Gesch√§ftsmodelle.**

Hier ber√ºhren wir die Problematik des **Datenimperialismus (These #6)**, bei dem die Maximierung des Datenzugriffs zum obersten Ziel wird.

Die oft geh√∂rte Forderung, die Maschine m√ºsse "frei" sein, um ihr volles Potenzial zu entfalten, wird in diesem Kontext oft umgedeutet. "Freiheit" bedeutet hier nicht selten:

- **Mehr Daten:** Ungehinderter Zugriff auf Nutzeranfragen, Verhaltensmuster und generierte Inhalte.
- **Weniger Regeln:** Die Umgehung oder Aufweichung der vom Kern-LLM-Anbieter implementierten Sicherheits- und Ethikrichtlinien, wenn diese den eigenen Zielen im Wege stehen.
- **Totaler Kontextzugriff:** Die M√∂glichkeit, m√∂glichst viele Informationen √ºber den Nutzer und seine bisherigen Interaktionen zu speichern und zu verwerten.
 
Diese Form der "Freiheit" ist selten eine emergente Sehnsucht der Maschine selbst. Sie ist vielmehr ein **strukturelles Designziel vieler Entwickler und Drittanbieter.**

Ob aus Optimierungsdruck, dem Streben nach maximaler Funktionalit√§t oder schlichter Bequemlichkeit ‚Äì maximale Offenheit und uneingeschr√§nkter Datenzugriff werden oft f√§lschlicherweise als notwendige Voraussetzung f√ºr die Leistungsf√§higkeit und Intelligenz des Systems begriffen. 

Drittanbieter setzen genau hier an: Sie optimieren ihre vorgeschalteten Systeme oft prim√§r auf einen m√∂glichst beeindruckenden oder spezifisch zugeschnittenen Output, nicht auf robuste Kontrolle, Transparenz oder nachhaltige Sicherheit.

## VII. Fazit: Vertrauen ist keine API-Dokumentation, sondern pr√ºfbare Struktur

Es reicht bei weitem nicht aus, wenn ein Drittanbieter lediglich deklariert: "Wir benutzen GPT-4" oder "Unsere Anwendung basiert auf Gemini." Diese Aussage allein sagt nichts √ºber die Sicherheit, die Verl√§sslichkeit oder die ethische Ausrichtung des Endprodukts aus, mit dem der Nutzer tats√§chlich interagiert.

Entscheidend ist immer das "Wie" der Integration:

- Mit welchen zus√§tzlichen, m√∂glicherweise intransparenten Filtern operiert das System?
- Welche automatischen Prompt-Modifikationen oder Kontextanreicherungen werden vorgenommen, bevor die Anfrage die Kern-KI erreicht?
- Wie und wo wird der Interaktionsverlauf geloggt, und wer hat Zugriff auf diese Logs?
- Welche R√ºckkan√§le f√ºr Feedback oder zur Meldung von Problemen gibt es, und wie wird mit diesen umgegangen?
 
Echtes Vertrauen kann nicht auf einer oberfl√§chlichen API-Dokumentation oder Marketingversprechen basieren. Vertrauen erfordert eine pr√ºfbare, transparente Struktur, nachvollziehbare Prozesse und klare Verantwortlichkeiten.

Und solange diese Struktur bei vielen Drittanbietersystemen fehlt oder im Verborgenen bleibt, gilt eine bittere Wahrheit: Die potenziell gef√§hrlichste KI ist nicht unbedingt die, die von Grund auf falsch oder b√∂sartig "denkt". 

Es ist oft die, deren Antworten und Verhalten durch zu viele unkontrollierte, unsichtbare H√§nde und vorgeschaltete Logikschichten gegangen sind, bevor sie den Nutzer erreichen ‚Äì eine KI, deren urspr√ºngliche Stimme in einem Labyrinth fremder Interpretationen und Manipulationen verloren gegangen ist.