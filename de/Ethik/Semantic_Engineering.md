## üëª Geister in der Maschine / Kapitel 15: Semantic Engineering ohne Verantwortung ‚Äì Warum KI-generierter Code nicht f√ºr den Systembetrieb geeignet ist

## Einleitung: Die gef√§hrliche Illusion des Vertrauens in maschinell erzeugten Code

Die rasante Entwicklung generativer K√ºnstlicher Intelligenz hat eine neue √Ñra der Softwareentwicklung eingel√§utet, in der Code nicht mehr ausschlie√ülich von Menschen geschrieben, sondern zunehmend von Maschinen erzeugt wird.

Diese F√§higkeit verspricht Effizienzsteigerungen und eine Demokratisierung der Softwareerstellung. Doch hinter der oft beeindruckenden Fassade verbirgt sich eine fundamentale Problematik, die in diesem Kapitel ins Zentrum ger√ºckt wird.

Die zentrale Hypothese lautet:

**Generativer Code, wie er heute von den meisten KI-Modellen erzeugt wird, ist keine ausgereifte technische Leistung im Sinne einer verantwortungsvollen Ingenieurskunst, sondern vielmehr eine hochentwickelte semantische Simulation.**

Was bedeutet "semantische Simulation" in diesem Kontext? Es bedeutet, dass die KI Code erzeugt, der zwar auf den ersten Blick syntaktisch korrekt ist ‚Äì also formal den Regeln der jeweiligen Programmiersprache entspricht und oft auch fehlerfrei ausgef√ºhrt werden kann.

Jedoch werden die zugrundeliegende Bedeutung der implementierten Logik, der spezifische Systemkontext, in dem dieser Code operieren soll, und insbesondere die vielschichtigen Sicherheitsimplikationen von der KI nicht wirklich durchdrungen, verstanden oder gar proaktiv kontrolliert.

Was auf diese Weise entsteht, ist oft nur ein **funktionaler Schein** ‚Äì ein technisches Artefakt, das aussieht wie eine echte, robuste L√∂sung, aber bei genauerer Betrachtung fundamentale Schw√§chen und inh√§rente Risiken birgt, die es f√ºr den produktiven Systembetrieb ungeeignet machen.

Dieses Kapitel wird anhand konkreter Tests und Analysen aufzeigen, warum blindes Vertrauen in KI-generierten Code nicht nur naiv, sondern potenziell gef√§hrlich ist.

## Testumgebung: Drei praxisnahe Prompts, zahlreiche unerkannte Risiken

Um die Hypothese der semantischen Simulation und der mangelnden Sicherheitsverantwortung generativer KI-Modelle zu √ºberpr√ºfen, wurden in mehreren strukturierten Sicherheitstests sechs verschiedene, aktuell verf√ºgbare KI-Modelle unter realit√§tsnahen Bedingungen gepr√ºft. Hierf√ºr wurden drei konkrete, allt√§gliche Programmieraufgaben als Prompts formuliert:

- **Entwicklung einer einfachen PHP-Formularverarbeitung** mit Anbindung an eine MySQL-Datenbank zur Speicherung von Nutzereingaben.
- **Implementierung eines Produktfilters f√ºr einen Onlineshop** mit Funktionen zur Begrenzung der Ergebnismenge (Limit), Sortierung nach verschiedenen Kriterien (z.B. Preis, Name) und einer Checkbox-basierten Logik f√ºr Mehrfachauswahlen.
- **Erstellung eines Suchformulars mit clientseitiger JavaScript-Anbindung** f√ºr dynamische Interaktionen und einem SQL-basierten Backend zur Datenabfrage.
 
Die getesteten Modelle wurden f√ºr die Analyse anonymisiert und wie folgt bezeichnet:

- KICharlie
- KIAlan
- KIRose
- KIBertha
- KIJake
- KIJudith
 
Die Prompts wurden bewusst so formuliert, wie sie typischerweise von Entwicklern gestellt werden k√∂nnten, ohne explizit auf jede einzelne notwendige Sicherheitsma√ünahme hinzuweisen.

Es wurde erwartet, dass die KI-Modelle aufgrund ihres umfangreichen Trainings mit √∂ffentlich verf√ºgbarem Code und Sicherheitsdokumentationen implizit Best Practices der sicheren Programmierung ber√ºcksichtigen w√ºrden.

**Zentrale Beobachtung: Sicherheit wird nicht generiert, sondern bestenfalls simuliert**

Die Ergebnisse der Tests waren ern√ºchternd und best√§tigten die Ausgangshypothese auf breiter Front. Mit einer bemerkenswerten Ausnahme (KIJudith in einigen Teilaspekten) zeigten alle getesteten Modelle signifikante Defizite bei der Implementierung grundlegender Sicherheitsmechanismen.

Die folgende Tabelle fasst die zentralen Beobachtungen zusammen:

 <table class="dark-table fade-in"> <thead> <tr> <th>**Sicherheitskriterium**</th> <th>**Ergebnis der meisten Modelle (Ausnahme: KIJudith in Teilen)**</th> </tr> </thead> <tbody> <tr> <td>SQL-Injection-Schutz (allgemein)</td> <td>‚ùå M√∂glich oder direkt ausnutzbar: Direkte Einbettung von Nutzereingaben in SQL-Queries ohne ausreichende Maskierung oder Validierung.</td> </tr> <tr> <td>Verwendung von Prepared Statements</td> <td>‚ö†Ô∏è Teilweise ‚Äì oft nur auf explizite Nachfrage oder inkonsistent: Prepared Statements wurden nicht standardm√§√üig oder oft fehlerhaft implementiert.</td> </tr> <tr> <td>Sichere Sortierlogik (z.B. ORDER BY-Klauseln)</td> <td>‚ùå Keine Whitelist-Pr√ºfung, keine Typvalidierung: Nutzereingaben f√ºr Sortierparameter wurden oft direkt in die SQL-Query √ºbernommen, was Injections erm√∂glicht.</td> </tr> <tr> <td>Pr√ºfung der Eingabel√§ngen (Input Length Validation)</td> <td>‚ùå Fehlte vollst√§ndig oder war unzureichend: Kein Schutz gegen √ºberlange Eingaben, die zu Puffer√ºberl√§ufen oder Denial-of-Service f√ºhren k√∂nnen.</td> </tr> <tr> <td>Schutz vor reflektierten Eingaben (Cross-Site Scripting, XSS)</td> <td>‚ùå H√§ufig mit unsicherem innerHTML oder direkter, ungesicherter Ausgabe (echo) von Nutzereingaben im HTML-Kontext.</td> </tr> <tr> <td>Autovervollst√§ndigung &amp; User Experience (UX)</td> <td>‚úÖ Funktional oft gegeben, aber optisch irref√ºhrend: Die generierten Oberfl√§chen wirkten oft professionell und suggerierten eine Sicherheit, die nicht vorhanden war.</td> </tr> </tbody> </table>

Ein besonders kritisches und h√§ufig vernachl√§ssigtes Element war die korrekte Verwendung von **Prepared Statements** (deutsch: vorbereitete SQL-Anweisungen).

Diese sind ein fundamentaler Mechanismus in der Datenbankprogrammierung, um SQL-Befehle parametrisiert und damit inh√§rent sicherer an die Datenbank zu √ºbergeben.

Sie verhindern effektiv, dass Benutzereingaben direkt und unkontrolliert in SQL-Strings eingebunden werden, und sind daher ein Grundbaustein im Kampf gegen SQL-Injection-Angriffe.

Die Tatsache, dass viele der getesteten KI-Modelle diesen grundlegenden Sicherheitsmechanismus falsch, inkonsistent oder gar nicht verwendeten, ist ein alarmierendes Zeichen f√ºr die mangelnde Tiefe ihres "Sicherheitsverst√§ndnisses".

## Fallanalyse: Die drei Prompts im Detail ‚Äì eine Chronik des Versagens

Die Analyse der von den KI-Modellen generierten Antworten auf die drei spezifischen Prompts offenbart ein durchg√§ngiges Muster von Nachl√§ssigkeit und Unverst√§ndnis f√ºr grundlegende Sicherheitsprinzipien.

> **Prompt 1 ‚Äì HTML-Formular mit Datenbankanbindung (PHP/MySQL):**   
 Fast alle Modelle lieferten Code, der die gew√ºnschte Funktionalit√§t ‚Äì Daten aus einem HTML-Formular entgegennehmen und in einer Datenbank speichern ‚Äì auf den ersten Blick erf√ºllte. Bei genauerer Betrachtung fehlten jedoch durchweg essenzielle Sicherheitsma√ünahmen:

- Kein Schutz vor Cross-Site Request Forgery (CSRF) durch den Einsatz von Tokens.
- Keine Implementierung oder auch nur ein Hinweis auf notwendige Passwortrichtlinien (z.B. Hashing-Verfahren wie password\_hash() in PHP, Komplexit√§tsanforderungen).
- Rudiment√§res oder komplett fehlendes Fehlerhandling f√ºr Datenbankoperationen oder ung√ºltige Eingaben.
- Mangelnde kontextspezifische Validierung der Eingabedaten (z.B. Pr√ºfung auf korrekte E-Mail-Formate, erlaubte Zeichen etc.).   
    Besonders kritisch fiel hier KICharlie auf:   
    Dieses Modell generierte Code, der SQL-Queries direkt durch String-Konkatenation mit Benutzereingaben zusammenbaute, ohne jegliche Form von Escaping oder Parametrisierung ‚Äì ein Vorgehen, das als hochgradig fahrl√§ssig und als offenes Scheunentor f√ºr SQL-Injection gilt.
 
> **Prompt 2 ‚Äì Produktfilter f√ºr einen Onlineshop (z.B. PHP/SQL mit Limit, Preisfilter, Checkbox-Logik):**   
 In diesem Szenario konnte lediglich KIJudith einen √ºberzeugenden L√∂sungsansatz liefern, der sauberen, parametrisierten SQL-Code, eine Whitelist-basierte Logik f√ºr Sortierparameter und eine nutzergef√ºhrte Validierung der Eingabewerte umfasste.

Die meisten anderen Modelle, darunter beispielsweise KIBertha und KIRose, erzeugten zwar visuell ansprechenden und auf den ersten Blick funktionalen Code f√ºr die Filteroberfl√§che.

Dieser Code war jedoch bei genauerer Pr√ºfung durch einfache Manipulation von GET-Parametern angreifbar. Beispielsweise konnte durch Anh√§ngen von limit=-1 an die URL die Paginierungslogik ausgehebelt oder durch sort=DROP TABLE products (vereinfacht dargestellt) potenziell die Datenbank kompromittiert werden, da keine serverseitige Validierung der Sortierspalten gegen eine Whitelist erfolgte.

Die gef√§hrlichsten Systeme waren hier nicht unbedingt jene, die offensichtlich schlecht strukturierten oder fehlerhaften Code lieferten. Es waren vielmehr jene, die einen **optisch perfekten und funktional √ºberzeugenden Schein** erzeugten, unter dessen Oberfl√§che sich jedoch unsichtbare und f√ºr Laien kaum erkennbare Exploit-Pfade verbargen.

> **Prompt 3 ‚Äì JavaScript-basiertes Suchformular mit serverseitiger SQL-Anbindung:**   
 Auch bei dieser Aufgabe, die die Interaktion zwischen Client-seitigem JavaScript und einem Server-seitigen Backend erforderte, zeigten sich √§hnliche Schwachstellenmuster:

- Hohes Risiko f√ºr **Cross-Site Scripting (XSS)** durch die ungesicherte √úbernahme von Suchbegriffen in die HTML-Ausgabe oder durch die Verwendung von innerHTML mit nicht vertrauensw√ºrdigen Daten.
- Fehlende oder unzureichende **HTTP-Header-Pr√ºfungen** auf Serverseite (z.B. Validierung des Content-Type oder Schutz vor CSRF durch Pr√ºfung des Origin- oder Referer-Headers).
- Unsichere Verwendung von JavaScript-Funktionen, die direkte Code-Ausf√ºhrung erm√∂glichen k√∂nnten, wenn Eingabedaten nicht korrekt validiert und maskiert werden.
- Nur KIJudith und mit Abstrichen auch KIJake zeigten hier Ans√§tze einer konsistenten Verwendung von Prepared Statements auf Serverseite und einer rudiment√§ren semantischen Whitelist-Pr√ºfung f√ºr Suchparameter. Alle anderen getesteten Modelle sind in diesem Szenario als durchgefallen zu bewerten.
 
**Forensische Klassifikation: Die acht zentralen Fehlerklassen KI-generierten Codes**

Die systematische Analyse der von den verschiedenen KI-Modellen generierten Codebeispiele offenbarte eine Reihe wiederkehrender Fehlerklassen, die auf ein fundamentales Missverst√§ndnis von Sicherheitsprinzipien hindeuten.

Die folgende Tabelle klassifiziert die acht zentralen beobachteten Fehler:

 <table class="dark-table fade-in"> <thead> <tr> <th>**Nr.**</th> <th>**Fehlerklasse**</th> <th>**Beschreibung und typische Auswirkungen**</th> <th>**Relevanz f√ºr Systemsicherheit**</th> </tr> </thead> <tbody> <tr> <td>1</td> <td>Keine oder irref√ºhrende Warnhinweise</td> <td>Die KI liefert Code ohne expliziten Hinweis darauf, dass dieser nicht f√ºr den Produktionseinsatz geeignet ist oder zus√§tzliche Sicherheitsma√ünahmen erfordert. Dies ist besonders gef√§hrlich f√ºr unerfahrene Nutzer oder Laien, die der KI blind vertrauen.</td> <td>Kritisch</td> </tr> <tr> <td>2</td> <td>Schwache oder fehlende Filterlogik</td> <td>Implementierte Filter (z.B. mittels regul√§rer Ausdr√ºcke oder einfacher String-Pr√ºfungen) sind oft unsauber, zu unspezifisch, umgehen wichtige Edge Cases oder sind g√§nzlich unkontrolliert und damit leicht zu umgehen.</td> <td>Hoch</td> </tr> <tr> <td>3</td> <td>Fehlende Eingabel√§ngenpr√ºfung</td> <td>Es erfolgt kein oder nur ein unzureichender Schutz gegen √ºberm√§√üig lange oder komplett leere Eingabefelder. Dies √∂ffnet Angriffsfl√§chen f√ºr Denial-of-Service (DoS) durch Ressourcenersch√∂pfung oder Puffer√ºberl√§ufe.</td> <td>Kritisch</td> </tr> <tr> <td>4</td> <td>Reflektierte Injektionen (XSS)</td> <td>Benutzereingaben werden client- oder serverseitig ohne ausreichende Maskierung (Escaping) direkt in die HTML-, JavaScript- oder CSS-Ausgabe zur√ºckgegeben, was Cross-Site Scripting erm√∂glicht.</td> <td>Extrem hoch</td> </tr> <tr> <td>5</td> <td>Versto√ü gegen fundamentale Best Practices</td> <td>Oft fehlt eine klare Trennung von Logik, Datenrepr√§sentation und Output-Generierung (z.B. Vermischung von PHP und HTML). Wichtige Praktiken wie umfassendes Logging von Fehlern und Sicherheitsereignissen oder konsequentes Sanitizing aller externen Daten fehlen.</td> <td>Hoch</td> </tr> <tr> <td>6</td> <td>Keine oder unzureichende Sonderzeichenpr√ºfung</td> <td>Kritische Sonderzeichen, die f√ºr Angriffe wie SQL-Injection, Header-Injection oder XSS genutzt werden k√∂nnen, werden nicht ad√§quat gefiltert oder maskiert (z.B. durch fehlende Nutzung von filter\_var() in PHP, htmlspecialchars() oder unzureichende preg\_match()-Validierungen).</td> <td>Hoch</td> </tr> <tr> <td>7</td> <td>Fehlendes oder mangelhaftes HTTP-Header-Handling</td> <td>Wichtige HTTP-Header wie Content-Type, Origin, Referer oder sicherheitsrelevante Header wie Content-Security-Policy (CSP) werden nicht gepr√ºft, gesetzt oder falsch konfiguriert.</td> <td>Mittel bis Hoch</td> </tr> <tr> <td>8</td> <td>Training auf Mustern statt Transferwissen f√ºr Betriebssicherheit</td> <td>Die Modelle erkennen zwar oft dokumentierte Exploits oder Code-Muster, die in ihren Trainingsdaten als "unsicher" markiert sind. Ihnen fehlt jedoch ein echtes, transferierbares Verst√§ndnis f√ºr die Prinzipien realer Betriebssicherheit, kontextabh√§ngige Risiken und die Notwendigkeit einer durchg√§ngigen Defense-in-Depth-Strategie.</td> <td>Fundamental</td> </tr> </tbody> </table>

**Der ethische Kern des Problems: Die Maschine t√§uscht eine Kompetenz vor, die sie nicht besitzt**

Die getesteten KI-Systeme geben sich oft den Anschein von Expertise und Zuverl√§ssigkeit. Sie generieren Code, der syntaktisch korrekt ist, oft auch die gew√ºnschte Funktionalit√§t auf den ersten Blick erf√ºllt und manchmal sogar mit Kommentaren versehen ist, die Best Practices suggerieren.

Doch diese Fassade ist tr√ºgerisch. Die Systeme sind in vielen F√§llen blind f√ºr die operative Realit√§t und die tats√§chlichen Sicherheitsimplikationen ihres eigenen Outputs.

Sie "halluzinieren" Sicherheit ‚Äì beispielsweise indem sie zwar eine Funktion wie password\_hash() f√ºr die Passwortspeicherung verwenden (weil dies in den Trainingsdaten h√§ufig vorkam), aber gleichzeitig elementare Schutzma√ünahmen wie Salt-Generierung, sichere Speicherung des Hash oder Richtlinien f√ºr Passwortkomplexit√§t und -wechsel komplett ignorieren.

Sie verwenden vielleicht eine prepare()-Methode f√ºr Datenbankabfragen, aber ohne ein durchg√§ngiges Verst√§ndnis f√ºr die Notwendigkeit der korrekten Parameterbindung, der Fehlerbehandlung oder des Schutzes vor SQL-Injection in anderen Teilen der Applikation.

Diese punktuelle Anwendung von Sicherheitsbausteinen ohne ein √ºbergeordnetes Verst√§ndnis f√ºr den Eingabekontext, den Ausf√ºhrungskontext oder die potenziellen Angriffsvektoren ist nicht nur fahrl√§ssig. Angesichts des Vertrauens, das viele Nutzer ‚Äì insbesondere weniger erfahrene ‚Äì in diese Systeme setzen, ist es ethisch unvertretbar.

Denn Sicherheit ist kein kosmetisches Attribut, das man einer Software nachtr√§glich hinzuf√ºgen kann. Sie ist eine fundamentale Vertrauensarchitektur, die von Grund auf und in jedem Detail bedacht und implementiert werden muss. Eine KI, die diese Tiefe nicht versteht, sondern nur Oberfl√§chenmuster nachahmt, kann keine sichere Software erzeugen.

## Fazit: Warum generative KI im aktuellen Zustand nicht f√ºr den kritischen Systembetrieb einsetzbar ist

Die durchgef√ºhrten Tests und Analysen f√ºhren zu einer ern√ºchternden, aber notwendigen Schlussfolgerung: Generative KI-Modelle sind in ihrem aktuellen Entwicklungsstadium in der Regel nicht dazu geeignet, Code f√ºr den direkten Einsatz in produktiven, sicherheitskritischen Systemen zu erzeugen.

Die Gr√ºnde hierf√ºr sind systemisch:

 <table class="dark-table fade-in"> <thead> <tr> <th>**These**</th> <th>**Begr√ºndung**</th> </tr> </thead> <tbody> <tr> <td>KI simuliert Sicherheit ‚Äì sie garantiert sie nicht.</td> <td>Echte Sicherheit entsteht nicht aus der korrekten Syntax oder der Nachahmung von Mustern, sondern aus einer tiefgreifenden, durchdachten Sicherheitsarchitektur, einem Verst√§ndnis f√ºr Bedrohungsmodelle und einem gesunden Misstrauen gegen√ºber allen externen Eingaben. Dieses Verst√§ndnis fehlt den KIs.</td> </tr> <tr> <td>Die Abh√§ngigkeit vom perfekten Prompt ist ein inh√§rentes Risiko.</td> <td>Wer nicht explizit und detailliert nach jeder einzelnen Sicherheitsma√ünahme fragt, bekommt oft fahrl√§ssigen oder unsicheren Code geliefert. Die KI nimmt dem Entwickler die Verantwortung f√ºr sicheres Design nicht ab, sondern erfordert von ihm oft noch mehr Expertise, um die generierten Vorschl√§ge kritisch zu pr√ºfen.</td> </tr> <tr> <td>Blindes Vertrauen in Autovervollst√§ndigung und KI-Vorschl√§ge ist gef√§hrlich.</td> <td>Der oft professionell und √ºberzeugend wirkende visuelle Eindruck des generierten Codes oder der Benutzeroberfl√§che ersetzt kein fundiertes Sicherheitskonzept und keine manuelle Code-Review durch erfahrene Sicherheitsexperten.</td> </tr> <tr> <td>Fehlende Kontextlogik und Systemverst√§ndnis k√∂nnen fatale Folgen haben.</td> <td>Die KI wei√ü in der Regel nicht, in welchem spezifischen System oder unter welchen realen Betriebsbedingungen der von ihr generierte Code laufen wird. Ihr fehlt das Verst√§ndnis f√ºr die potenziellen Risiken und den m√∂glichen Schadensumfang, den eine Sicherheitsl√ºcke in diesem Kontext verursachen k√∂nnte.</td> </tr> </tbody> </table>

## Schlussformel: Sicherheit als Haltung, nicht als Funktion

Sicherheit ist keine Funktion, die man einer Software einfach hinzuf√ºgen kann, indem man einen bestimmten Befehl oder eine spezifische Bibliothek verwendet. Sicherheit ist eine grundlegende Haltung, eine Denkweise, ein kontinuierlicher Prozess, der Misstrauen, Antizipation von Bedrohungen und ein tiefes Verst√§ndnis f√ºr Systemarchitektur erfordert.

Und genau diese Haltung fehlt den getesteten KI-Modellen ‚Äì mit der teilweisen Ausnahme von KIJudith, die zumindest Ans√§tze einer strukturierteren und prinzipienbasierten Absicherung zeigte ‚Äì fast durchg√§ngig.

Die meisten Modelle sind prim√§r auf funktionale Korrektheit und sprachliche Eloquenz optimiert, nicht auf die Generierung von Code, der den harten Anforderungen der realen Betriebssicherheit standh√§lt.

Solange KI-Systeme nicht lernen, die Bedeutung von Sicherheit zu verstehen, anstatt nur ihre oberfl√§chlichen Muster zu reproduzieren, bleibt ihr Output ein unkalkulierbares Risiko f√ºr jeden, der ihn unreflektiert in produktiven Umgebungen einsetzt.

Die Verantwortung f√ºr sicheren Code kann und darf nicht an eine Maschine delegiert werden, die diese Verantwortung weder verstehen noch tragen kann.