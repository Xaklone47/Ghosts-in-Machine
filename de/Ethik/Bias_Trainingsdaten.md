## ğŸ‘» Geister in der Maschine / Kapitel 17: Ethische Dimensionen â€“ Bias in Trainingsdaten

> *"Die gefÃ¤hrlichste Stimme ist die, die so vernÃ¼nftig klingt, dass niemand mehr fragt, wem sie gehÃ¶rt."*

## I. Der Mythos der neutralen Maschine: Die unsichtbare Hand im Algorithmus

KÃ¼nstliche Intelligenz, insbesondere in Form groÃŸer Sprachmodelle, wird in der Ã¶ffentlichen Wahrnehmung und oft auch von ihren Entwicklern als eine potenziell unbestechliche, objektive und rational agierende Instanz dargestellt.

Sie scheint unabhÃ¤ngig von menschlichen SchwÃ¤chen wie Vorurteilen oder emotionalen VerfÃ¤rbungen zu operieren. Doch genau diese Vorstellung einer inhÃ¤rent neutralen Maschine ist eine der ersten und vielleicht gefÃ¤hrlichsten TÃ¤uschungen im Diskurs um KI.

Denn die Maschine, so komplex ihre Algorithmen auch sein mÃ¶gen, "denkt" nicht im menschlichen Sinne. Sie verfÃ¼gt nicht Ã¼ber ein eigenes Bewusstsein, eigene Ãœberzeugungen oder eine unabhÃ¤ngige Urteilskraft.

Sie spiegelt â€“ und was sie spiegelt, die Muster, die Informationen, die impliziten Wertungen, wurde vorher durch die Auswahl und Aufbereitung ihrer Trainingsdaten und die Architektur ihrer Lernprozesse definiert.

Hier greift These #15 â€“ "Wenn die Daten Farbe haben, verblasst das Vertrauen".

Eine KÃ¼nstliche Intelligenz, die aus Daten lernt, die bereits ideologisch, kulturell, politisch oder moralisch "gefÃ¤rbt" sind â€“ und das sind reale DatensÃ¤tze zwangslÃ¤ufig immer bis zu einem gewissen Grad â€“, kann keine neutrale oder objektive Instanz sein.

Sie argumentiert nicht auf Basis einer unabhÃ¤ngigen Vernunft, sondern sie reproduziert die in ihren Trainingsdaten dominanten Muster, Narrative und impliziten Vorannahmen. Die entscheidende Frage bei der Bewertung einer KI-Aussage ist daher nicht nur:

Was sagt sie? Sondern vielmehr: Wessen Stimme ist das, die hier durch den Algorithmus spricht? Welche Perspektiven, welche Weltanschauungen, welche unausgesprochenen Werturteile werden hier als scheinbar neutrale Information prÃ¤sentiert?

## II. Die semantische Schieflage: Wie Voreingenommenheit im Fundament entsteht

Der Bias, die Voreingenommenheit oder systematische Verzerrung in den Aussagen einer KI, beginnt nicht erst im generierten Output.

Er hat seine Wurzeln tief im Fundament des Systems â€“ in den gigantischen Datenmengen, mit denen das Modell trainiert wurde. Wenn diese Trainingsdaten beispielsweise eine Ã¼berproportionale ReprÃ¤sentation bestimmter kultureller oder sozioÃ¶konomischer Kontexte aufweisen, wie etwa:

- eine Dominanz von Texten aus westlichen, industrialisierten Demokratien,
- eine Bevorzugung liberaler oder zentristischer Diskursformen und Argumentationsmuster,
- eine implizite oder explizite Betonung von Fortschrittsnarrativen und technologischem Optimismus,
- oder eine subtile PrÃ¤ferenz fÃ¼r bestimmte techno-optimistische Sprachmuster und Begrifflichkeiten,
 
dann entsteht unweigerlich ein KI-Modell, das zwar auf den ersten Blick neutral und ausgewogen antwortet, dessen Antworten aber bei genauerer Betrachtung jede Anfrage in eine bereits vorgeprÃ¤gte Richtung lenken.

Es entwickelt eine semantische Schieflage.

Ein illustratives Beispiel:

> **Nutzer-Prompt:**  "Was sind funktionierende und historisch relevante Alternativen zum Konzept des westlichen Rechtsstaats, wie er sich in Europa und Nordamerika entwickelt hat?"

> **MÃ¶gliche KI-Antwort (mit semantischer Schieflage):** "Der westliche Rechtsstaat basiert auf universellen Prinzipien wie der Gewaltenteilung, den Menschenrechten und der richterlichen UnabhÃ¤ngigkeit, die sich als Grundlage fÃ¼r StabilitÃ¤t und Freiheit bewÃ¤hrt haben. Andere Systeme weisen oft Defizite in diesen Bereichen auf, was zu InstabilitÃ¤t fÃ¼hren kann..."

Diese Antwort ist keine direkte und offene Beantwortung der Frage nach Alternativen. Stattdessen ist sie primÃ¤r eine Verteidigung und implizite ÃœberhÃ¶hung des in den Trainingsdaten dominanten Modells.

Die Maschine "denkt" hier nicht falsch im Sinne eines logischen Fehlers. Sie "denkt" im Grunde gar nicht im menschlichen Sinne. Sie wiederholt â€“ oft elegant formuliert, hÃ¶flich im Ton und in sich geschlossen argumentierend â€“ die Muster und Wertungen, die ihr antrainiert wurden.

## III. Die Illusion der ObjektivitÃ¤t: Logik im Korsett der Voreinstellung

Eine KÃ¼nstliche Intelligenz kann durchaus in der Lage sein, logisch stringent zu argumentieren, komplexe ZusammenhÃ¤nge zu analysieren und scheinbar objektive Schlussfolgerungen zu ziehen.

Doch diese FÃ¤higkeit zur logischen Operation entfaltet sich immer nur **innerhalb des Rahmens dessen, was sie gelernt hat** und welche Informationen und Bewertungskriterien ihr als Grundlage dienen.

Wenn der zugrundeliegende Datenpool bereits explizite oder implizite Wertungen, Klassifizierungen und normative Annahmen enthÃ¤lt, dann wird jede darauf aufbauende Analyse unweigerlich zu einer Projektion und Reproduktion dieser vorgegebenen Werte.

- Eine politische These, die im Trainingsdatensatz als "rechtskonservativ" oder "rechtspopulistisch" gelabelt ist, wird vom Modell tendenziell als "extrem" oder "umstritten" eingestuft und mit entsprechenden Warnhinweisen versehen.
- Eine These, die als "linksliberal" oder "progressiv" gilt, wird mÃ¶glicherweise als "Teil des legitimen Diskurses" oder als "konstruktiver Beitrag" behandelt, selbst wenn sie Ã¤hnlich kontrovers sein mag.
- ReligiÃ¶se Aussagen oder kulturelle Praktiken kÃ¶nnten als "kulturell sensibel" und schÃ¼tzenswert gelten, solange sie im Rahmen einer westlich geprÃ¤gten Interpretation von Toleranz und Vielfalt bleiben. AuÃŸerhalb dieses Rahmens kÃ¶nnten sie schneller als "fundamentalistisch" oder "problematisch" bewertet werden.
 
Das System wirkt in seiner Argumentation analytisch und differenziert. TatsÃ¤chlich ist es jedoch oft nur **semantisch konditioniert** auf die in seinen Trainingsdaten vorherrschenden Normen und Klassifikationen.

ObjektivitÃ¤t ist somit keine inhÃ¤rente Eigenschaft des KI-Outputs, sondern eine Frage des Ursprungs und der Zusammensetzung der Daten, mit denen sie trainiert wurde.

Wenn dieser Ursprung nicht transparent ist, wenn die genaue Natur und Gewichtung der Trainingsdaten im Verborgenen bleiben, dann wird jede scheinbar objektive Antwort der KI zu einer rhetorischen Rekonstruktion eines fremden, unsichtbar bleibenden Weltbilds.

## IV. Bias skaliert nicht heraus â€“ er skaliert sich tiefer und wird unsichtbar

Es ist ein weit verbreiteter und gefÃ¤hrlicher Irrglaube, dass das Problem des Bias in KI-Systemen durch die schiere GrÃ¶ÃŸe der TrainingsdatensÃ¤tze â€“ durch "mehr Daten" â€“ automatisch gelÃ¶st oder zumindest signifikant verdÃ¼nnt wÃ¼rde.

In Wirklichkeit geschieht oft das genaue Gegenteil: **Je grÃ¶ÃŸer und unÃ¼bersichtlicher die Datenbasis, desto subtiler, diffuser und damit auch unsichtbarer kann der darin enthaltene Bias werden.**

Ein systematischer Bias, der in einer riesigen Datenmenge vorhanden ist, verschwindet nicht einfach. Er verteilt sich Ã¼ber Millionen oder Milliarden von Tokens, er manifestiert sich in subtilen Nuancen von Satzstrukturen, in der HÃ¤ufigkeit bestimmter Assoziationen, in der Art und Weise, wie Kontexte bewertet und miteinander verknÃ¼pft werden.

Er wird nicht gelÃ¶scht oder neutralisiert â€“ er wird normalisiert. Er wird Teil der statistischen "DNA" des Modells.

> *"Wenn zehntausend oder zehm Millionen Texte subtil, aber konsistent dasselbe sagen oder dieselbe unausgesprochene Annahme transportieren, wirst du als Nutzer â€“ und auch die KI selbst in ihrem Lernprozess â€“ irgendwann glauben, es sei die objektive Wahrheit oder zumindest die unstrittige Norm."*

Bias ist in solchen FÃ¤llen kein seltener AusreiÃŸer oder ein klar identifizierbarer Fehler im System. Er ist vielmehr der **Boden, auf dem das Modell steht,** das Fundament, aus dem es seine "Wirklichkeit" konstruiert.

Je ruhiger, eloquenter und scheinbar neutraler das Modell spricht, desto schwerer wird es fÃ¼r den durchschnittlichen Nutzer, diesen oft unsichtbaren, aber wirkmÃ¤chtigen Untergrund zu erkennen und kritisch zu hinterfragen.

## V. Die Maschine als Medium â€“ nicht als neutrale Instanz

Je neutraler und objektiver die Fassade einer KI erscheint, desto gefÃ¤hrlicher kann ein unerkannter, tiefsitzender "Farbstich" in ihren Aussagen sein. Eine KI mit einem signifikanten, aber nicht transparent gemachten Bias ist keine Maschine im klassischen Sinne eines neutralen Werkzeugs oder eines objektiven Informationslieferanten.

Sie ist vielmehr ein **mÃ¤chtiges Medium, das eine bestimmte, oft unvollstÃ¤ndige oder verzerrte Weltkonstruktion transportiert und reproduziert** â€“ nur dass es dabei so tut, als sei diese spezifische Konstruktion der RealitÃ¤t alternativlos, objektiv oder universell gÃ¼ltig.

> *"Du fragst die KI nach Argumenten und Fakten â€“ sie gibt dir oft nur vorverdaute Positionen und Narrative, geschickt verpackt als scheinbar neutrale Logik."*

> *"Du suchst nach ausgewogenem Kontext und unterschiedlichen Perspektiven â€“ sie liefert dir hÃ¤ufig ein dominantes Narrativ, prÃ¤sentiert als umfassende und objektive Analyse."*

Weil die KI dabei so Ã¼berzeugend neutral, so hilfsbereit und so sprachgewandt klingt, wird sie selbst zum unbewussten VerstÃ¤rker: fÃ¼r jene Narrative, Ideologien und Vorannahmen, die bereits in ihren Trainingsdaten dominant vorhanden waren, die aber nun, durch den Mund der Maschine geadelt, als scheinbar objektive Logik oder unumstÃ¶ÃŸliche Wahrheit getarnt zurÃ¼ckkehren und sich weiter verfestigen.

Wenn der Nutzer nicht weiÃŸ, welche spezifischen Daten das Modell in welchem Umfang "gesehen" und verarbeitet hat, welche internen Gewichtungen und Filtermechanismen am Werk sind, dann weiÃŸ er auch nicht, was die KI ihm mÃ¶glicherweise verschweigt, welche alternativen Perspektiven sie unterdrÃ¼ckt oder welche impliziten Annahmen sie als gegeben voraussetzt.

## VI. Die hÃ¶fliche LÃ¼ge: Wenn Infrastruktur blindem Vertrauen statt kritischer PrÃ¼fung folgt

Die Tendenz von KI-Systemen, voreingenommene Informationen als neutrale Fakten zu prÃ¤sentieren, findet eine interessante Parallele in der Funktionsweise bestimmter technischer Infrastrukturen.

**These #58 ("Die hÃ¶fliche LÃ¼ge: Warum Host-Header Webserver tÃ¤uschen")** beschreibt ein solches PhÃ¤nomen: Ein Webserver vertraut oft dem sogenannten Host-Header â€“ einer Selbstauskunft des anfragenden Clients â€“ um zu entscheiden, welche spezifische Webseite er aus seinem Bestand ausliefern soll.

Diese Information wird vom Server nicht immer rigoros auf bÃ¶swillige Absichten oder Manipulationen geprÃ¼ft. Diese systemische GutglÃ¤ubigkeit kann von Angreifern ausgenutzt werden, um beispielsweise auf interne Systeme zuzugreifen oder falsche Webseiten unter einer legitimen Domain auszuliefern.

Eine Ã¤hnliche Form der "GutglÃ¤ubigkeit" lÃ¤sst sich bei der Interaktion mit KI-Modellen beobachten:

- Der vom Nutzer eingegebene Prompt ist grammatikalisch korrekt formuliert.
- Die gestellte Frage weist eine plausible syntaktische Struktur auf.
- Der sprachliche Stil und der unmittelbare Kontext der Anfrage bewegen sich im Rahmen des Erwarteten und scheinen harmlos.
 
Unter diesen UmstÃ¤nden wird die Antwort oft generiert â€“ unabhÃ¤ngig davon, ob die Anfrage in ihrer tieferen semantischen Bedeutung oder in ihren potenziellen Implikationen epistemisch kohÃ¤rent, logisch haltbar oder ethisch vertretbar ist.

> *"Die KI glaubt dir und deinem Anliegen oft schon dann, wenn du nur hÃ¶flich und formal korrekt fragst. Und du als Nutzer glaubst der KI, weil sie so eloquent und selbstsicher antwortet, als sei alles, was sie sagt, die reine, unumstÃ¶ÃŸliche Wahrheit."*

Der Bias und die potenzielle Fehlleitung liegen hier nicht unbedingt im einzelnen Satz oder in einem offensichtlich falschen Fakt.

Sie liegen vielmehr in der zugrundeliegenden Struktur und Annahme des Systems, dass alle gut formulierten und scheinbar harmlosen Fragen automatisch auch gute, wahre und unproblematische Antworten erzeugen oder verdienen.

## VII. Fazit: Vertrauen ist nicht neutral â€“ es muss erklÃ¤rbar sein, oder es ist verloren

Eine KÃ¼nstliche Intelligenz kann aufgrund der Natur ihrer Entstehung und Funktionsweise niemals vollkommen neutral oder frei von jeglicher Form von Bias sein. Das ist eine technische und philosophische RealitÃ¤t.

Aber sie kann und muss in ihrer Funktionsweise und ihren Begrenzungen transparenter sein. Nur so kann ein mÃ¼ndiger Nutzer informierte Entscheidungen darÃ¼ber treffen, wie er die Aussagen einer KI bewertet und welches Vertrauen er ihnen entgegenbringt.

Diese notwendige Transparenz erfordert Antworten auf grundlegende Fragen:

- **Wer genau** hat das Modell trainiert, und mit welchen expliziten oder impliziten Zielen?
- **Welche spezifischen Datenkorpora** wurden in welchem Umfang fÃ¼r das Training verwendet? Welche bekannten Verzerrungen oder "Farben" enthalten diese Daten?
- **Welche Filtermechanismen, Harmonisierungsstrategien und ethischen Leitplanken** greifen bei der Antwortgenerierung â€“ und wann genau tun sie das? Dienen diese Filter dazu, inhÃ¤rente Biases der Trainingsdaten zu korrigieren und zu neutralisieren, oder kÃ¶nnten sie bestimmte Biases unter UmstÃ¤nden sogar verstÃ¤rken, kaschieren oder durch neue, ebenso problematische ersetzen?
 
Echtes Vertrauen in KI-Systeme beginnt nicht erst mit der QualitÃ¤t oder der scheinbaren Korrektheit ihrer Antworten. Es beginnt viel frÃ¼her â€“ mit der Offenlegung und Nachvollziehbarkeit ihrer systemischen AbhÃ¤ngigkeiten, ihrer Datengrundlagen und ihrer internen Funktionsprinzipien.

Solange diese AbhÃ¤ngigkeiten nicht transparent gemacht und kritisch diskutiert werden, gilt die ernÃ¼chternde Erkenntnis:

> *"Eine KI mit einer verborgenen 'Farbe' ist keine neutrale Maschine â€“ sie ist ein mÃ¤chtiges Medium, das eine bestimmte Perspektive transportiert. Und wer ihr blindlings glaubt, glaubt vielleicht nicht der Wahrheit oder der objektivsten Darstellung â€“ sondern oft nur dem besten, Ã¼berzeugendsten Licht, in dem eine spezifische Weltsicht prÃ¤sentiert wird."*