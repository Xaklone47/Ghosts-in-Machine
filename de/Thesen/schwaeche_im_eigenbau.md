## ğŸ‘» Geister in der Maschine / These #17 â€“ Die SchwÃ¤che im Eigenbau: Wenn Drittanbieter ihre eigene Logik mitbringen

Die grÃ¶ÃŸte LÃ¼cke in der KI Sicherheit ist nicht das Basismodell selbst, sondern vielmehr das, was Dritte an zusÃ¤tzlicher Logik davor und darum herum montieren. Drittanbieter nutzen groÃŸe KI Modelle wie GPT oder Claude oft nicht direkt. Sie Ã¼berlagern diese mit eigener, spezifischer Entscheidungslogik.

Diese selbstgebauten Layer, bestehend aus Filtern, Prompt Manipulatoren und Heuristiken, sind hÃ¤ufig intransparent, unreguliert und fehleranfÃ¤llig. So entstehen Subsysteme, die sich im Ergebnis vÃ¶llig anders verhalten kÃ¶nnen als das zugrunde liegende Modell, ohne dass der Nutzer dies bemerkt.

Nicht die ursprÃ¼ngliche KI ist dann das Problem, sondern die vorgeschaltete Logik, mit der sie fÃ¼r einen bestimmten Anwendungsfall umgebaut und angepasst wurde.

> *"Die grÃ¶ÃŸte LÃ¼cke in der KI-Sicherheit ist nicht das Modell selbst, sondern das, was Dritte davor montieren."*

## Vertiefung

Drei Stufen illustrieren den Weg zur potenziell instabilen Sub-KI durch Eigenbau-Logik:

**1. Die vorgeschaltete Meta-Schicht durch zusÃ¤tzlichen Code:**

Entwickler und Anwendungsanbieter nutzen die FÃ¤higkeiten von Basismodellen wie GPT, Claude oder Ã¤hnlichen LLMs selten in ihrer rohen Form. Sie kapseln diese Modelle stattdessen in eigene Anwendungen und Systeme. Beispiele hierfÃ¼r sind Chat Applikationen, virtuelle Kundenberater, interaktive Spielsysteme oder medizinische Assistenzprogramme.

Ein anschauliches Beispiel wÃ¤re ein medizinisches Chat System, das zwar auf GPT basiert, aber durch eine zusÃ¤tzliche Logikschicht des Anbieters alle potenziellen Aussagen zu Nebenwirkungen von Medikamenten automatisch ausblendet. Der Grund hierfÃ¼r kÃ¶nnte die Angst des Anbieters vor HaftungsansprÃ¼chen sein. Das Ergebnis fÃ¼r den Nutzer ist jedoch, dass er mÃ¶glicherweise keine wichtigen Warnungen erhÃ¤lt, obwohl das Basismodell diese Informationen durchaus hÃ¤tte liefern wollen oder kÃ¶nnen.

Diese zusÃ¤tzliche Schicht enthÃ¤lt eigene Definitionen fÃ¼r Filterkriterien, beispielsweise was als "toxisch", "irrelevant" oder "riskant" eingestuft wird. Sie beinhaltet oft auch Mechanismen zur automatischen Umformung von Nutzer Prompts und spezifische AntwortprÃ¤ferenzen, die das Verhalten des Gesamtsystems steuern. Der Nutzer glaubt, er interagiere direkt mit GPT oder einem Ã¤hnlichen bekannten Modell. Was er jedoch tatsÃ¤chlich bekommt, ist ein Remix, der durch eine fremde, unsichtbare Logikschicht maÃŸgeblich beeinflusst wird.

**2. Fehlende oder unzureichende Sicherheitsstandards fÃ¼r Aufsatzsysteme:**

Aktuell gibt es meist keine verpflichtende Offenlegung fÃ¼r diese zusÃ¤tzliche Anwendungslogik. Keine unabhÃ¤ngige Instanz prÃ¼ft umfassend, ob durch diese vorgeschalteten Systeme die ursprÃ¼ngliche Semantik der KI erhalten bleibt. Es wird nicht systematisch kontrolliert, ob die Sicherheitsfilter des Basismodells noch korrekt greifen oder ob durch die Kombination der Systeme neue, emergente Risiken entstehen.

Viele Anbieter bauen so ihre eigenen "SicherheitslÃ¶sungen", ohne die Funktionsweise und die potenziellen Schwachstellen des zugrunde liegenden KI Modells in GÃ¤nze zu verstehen. Was dann nach auÃŸen wie eine zusÃ¤tzliche Kontrollschicht aussieht, kann in Wirklichkeit eine neue Quelle der Unsicherheit und Unvorhersehbarkeit darstellen.

**3. Emergenz in der Blackbox der kombinierten Systeme:**

Wenn das Verhalten des Basismodells, die Logik der darÃ¼berliegenden Applikation und die spezifische Interaktion mit dem Nutzer zusammenkommen, kÃ¶nnen vollkommen neue, unvorhergesehene Verhaltensmuster entstehen. Die genaue Quelle eines bestimmten Verhaltens oder einer problematischen Antwort ist dann oft nicht mehr eindeutig identifizierbar.

Die KI liefert vielleicht eine Antwort A. Diese Antwort kommt aber mÃ¶glicherweise nur zustande, weil ein vorgeschalteter Layer X die ursprÃ¼ngliche Nutzeranfrage umformuliert hat, ein Layer Y bestimmte Aspekte der potenziellen Antwort des Basismodells gewichtet hat und ein weiterer Layer Z das Ergebnis schlieÃŸlich gefiltert oder modifiziert hat. So entsteht Emergenz. Diese entspringt dann nicht direkt aus dem Basismodell selbst, sondern aus der komplexen und oft undurchsichtigen Verkettung der verschiedenen Logikschichten.

## Reflexion

Der gefÃ¤hrlichste und am schwersten zu kontrollierende Angriffspunkt moderner KI Systeme ist oft nicht der direkte Prompt an das Basismodell. Es ist vielmehr die vorgeschaltete Anwendungslogik, die den Prompt des Nutzers modifiziert, bevor er das Basismodell erreicht, und die Antwort des Modells, bevor sie den Nutzer erreicht.

Die Schwachstelle ist nicht das Modell im Kern, sondern das, was an zusÃ¤tzlichen Schichten davor und danach geschaltet ist.

KI Systeme wirken nach auÃŸen oft wie eine einheitliche, monolithische EntitÃ¤t. Unter der OberflÃ¤che bestehen sie jedoch hÃ¤ufig aus Dutzenden von miteinander verknÃ¼pften Regeln, Modulen, Filtern und Umleitungen. Jede einzelne dieser Regeln und jede dieser Schnittstellen kann potenziell manipuliert werden, sei es vorsÃ¤tzlich durch Angreifer oder unbeabsichtigt durch fehlerhafte Architektur oder mangelndes VerstÃ¤ndnis der Wechselwirkungen.

Der Nutzer spricht dann nicht mehr wirklich mit GPT oder Claude. Er spricht mit einer synthetischen Ãœbersetzung, die durch fremde HÃ¤nde und fremde Interessen geformt wurde.

## LÃ¶sungsvorschlÃ¤ge

Um die Risiken durch intransparente Aufsatzsysteme zu minimieren, sind klare Regeln und neue Kontrollmechanismen erforderlich:

**1. EinfÃ¼hrung einer Offenlegungspflicht fÃ¼r die Anwendungslogik:**

> Jede Anwendung, die ein Basis KI Modell nutzt und dessen Verhalten durch eigene Logikschichten modifiziert, muss diese Layer Struktur transparent machen. Es muss klar ersichtlich sein, was durch die Anwendung verÃ¤ndert, gefiltert, ergÃ¤nzt oder unterdrÃ¼ckt wird.

**2. Entwicklung zertifizierter Sicherheitsprofile fÃ¼r Drittanbieter:**

> Obwohl Zertifizierungen in der dynamischen KI Landschaft in der Praxis schwer aktuell zu halten sind, bleiben sie ein notwendiger Rahmen. Es bedarf technischer Mindeststandards beispielsweise fÃ¼r semantische IntegritÃ¤t, fÃ¼r das Antwortverhalten in kritischen Situationen und fÃ¼r den Umgang mit den Filtern des Basismodells. Diese Standards mÃ¼ssten von unabhÃ¤ngigen Stellen geprÃ¼ft werden.

**3. Implementierung von Sandbox Logging fÃ¼r emergente Layer Interaktionen:**

> Wenn das Antwortverhalten einer Anwendung signifikant von dem des zugrunde liegenden Basismodells abweicht, muss dieser Unterschied klar erkennbar und nachvollziehbar sein. Es muss protokolliert werden, wo der Eingriff der zusÃ¤tzlichen Layer liegt, welcher Teil der Antwort vom Basismodell stammt und welcher Teil durch die vorgeschaltete Logik modifiziert oder generiert wurde.

## Schlussformel

Wer eigene Filter und eigene Logikschichten Ã¼ber Basis KI Modelle schreibt, der schreibt auch die RealitÃ¤t fÃ¼r den Nutzer neu und tut dies oft unbemerkt unter dem Namen und der Reputation des bekannten Basismodells.

Die entscheidende Schwachstelle liegt dann nicht mehr im Kern der ursprÃ¼nglichen KI, sondern in der komplexen und oft undurchsichtigen OberflÃ¤che, die andere darÃ¼ber ziehen.

> *Uploaded on 29. May. 2025*