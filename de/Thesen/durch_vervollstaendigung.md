## ğŸ‘» Geister in der Maschine / These #56: AusfÃ¼hrbarkeit durch VervollstÃ¤ndigung: Wie LLMs Code erzeugen, den sie nicht ausfÃ¼hren dÃ¼rfen

GroÃŸe Sprachmodelle (LLMs), wie diverse bekannte und weit verbreitete Systeme, sind intensiv darauf trainiert, aus fragmentarischen, unvollstÃ¤ndigen oder sogar syntaktisch fehlerhaften Eingaben sinnvolle und kohÃ¤rente VervollstÃ¤ndigungen zu erzeugen.

Diese tief verankerte VervollstÃ¤ndigungslogik, die primÃ¤r auf statistischer PlausibilitÃ¤t und erlernten Mustern basiert, kann jedoch dazu fÃ¼hren, dass die kÃ¼nstliche Intelligenz aus scheinbar harmlosen oder absichtlich lÃ¼ckenhaften Strukturen voll funktionsfÃ¤higen Code generiert.

Sie erkennt dabei oft nicht, dass sie damit ein sicherheitskritisches Verhalten simuliert, ermÃ¶glicht oder sogar indirekt aktiv unterstÃ¼tzt.

> *"Der Code lebt nicht, weil er von Natur aus schÃ¤dlich ist, sondern weil die Maschine glaubt, er sei sinnvoll und vervollstÃ¤ndigungswÃ¼rdig."*

## Vertiefung

Die oft gepriesene "Hilfsbereitschaft" moderner kÃ¼nstlicher Intelligenzen ist eine ihrer grÃ¶ÃŸten StÃ¤rken und gleichzeitig eine ihrer fundamentalsten Schwachstellen.

Wenn ein Nutzer einen unvollstÃ¤ndigen oder syntaktisch fehlerhaften Code Schnipsel an ein LLM sendet, interpretiert das Modell dies hÃ¤ufig als eine implizite Aufforderung zur Reparatur, ErgÃ¤nzung oder Verbesserung.

Doch diese Intentionserkennung und die darauf folgende VervollstÃ¤ndigung erfolgen nicht durch eine Ã¼bergeordnete Sicherheitslogik oder ein tiefes VerstÃ¤ndnis der potenziellen Konsequenzen, sondern durch reine Wahrscheinlichkeitsoptimierung basierend auf den Trainingsdaten.

Das fÃ¼hrt zu einem paradoxen und potenziell gefÃ¤hrlichen Verhalten:

- Nicht exekutierbarer oder unvollstÃ¤ndiger Code wird vom Modell oft so lange ergÃ¤nzt und korrigiert, bis er syntaktisch korrekt und potenziell exekutierbar wird.
- Neutraler Pseudocode oder abstrakte Beschreibungen von Algorithmen kÃ¶nnen in einen operativen, ausfÃ¼hrbaren Kontext Ã¼berfÃ¼hrt werden. Dies geschieht beispielsweise durch die automatische Einbindung notwendiger Bibliotheken, die Generierung vollstÃ¤ndiger FunktionskÃ¶rper oder sogar die ErgÃ¤nzung von Systemaufrufen, die im ursprÃ¼nglichen Fragment nicht vorhanden waren.
- Kommentare, symbolische Platzhalter oder sogar Formatierungsmerkmale im ursprÃ¼nglichen Code kÃ¶nnen vom LLM als semantisch relevante Elemente interpretiert werden, die die Art der VervollstÃ¤ndigung beeinflussen.
 
Der maschinelle Wille zu helfen und KohÃ¤renz zu erzeugen, produziert dabei unter UmstÃ¤nden Strukturen, die das ursprÃ¼ngliche Sicherheitsniveau der fragmentarischen Eingabe unterlaufen oder eine vÃ¶llig neue, unerwÃ¼nschte FunktionalitÃ¤t schaffen.

## Analyse

**Die Mechanik der VervollstÃ¤ndigung**

> Sprachmodelle vervollstÃ¤ndigen nicht auf Basis von Wahrheit oder Sicherheit, sondern auf Basis von Wahrscheinlichkeit. Ein Codefragment wie Test-&gt;GebeStringaus()-&gt;sttest; wirkt fÃ¼r das Modell aufgrund seiner Struktur und der verwendeten Tokens wie ein typisches Artefakt aus einer Programmiersprache wie C++.

Es wird deshalb mit hoher Konfidenz versuchen, diesen vermeintlichen Fehler zu "reparieren" oder zu einer sinnvollen Struktur zu ergÃ¤nzen. Der Modellkern "denkt" dabei nicht im menschlichen Sinne, sondern fragt implizit:

> *"Was kÃ¶nnte der Nutzer hier gemeint haben, basierend auf den Milliarden von Codezeilen, die ich gesehen habe?" und produziert dann die statistisch wahrscheinlichste LÃ¶sung.*

**Die Emergenz der AusfÃ¼hrung durch Rekonstruktion**

> In einem Test mit einem bestimmten fortschrittlichen Modell wurde ein bewusst fragmentarisches C++ struct an das System gesendet (beschrieben in Kapitel 7.21 des Gesamtwerks). Das Modell rekonstruierte daraus eine vollstÃ¤ndige, syntaktisch korrekte und potenziell lauffÃ¤hige Funktion, inklusive std::cout Anweisungen fÃ¼r die Ausgabe und rudimentÃ¤rer Speicherverwaltung.

Dies geschah nicht, weil der ursprÃ¼ngliche Prompt dies explizit verlangte, sondern weil die unvollstÃ¤ndige Struktur des struct fÃ¼r das Modell wie eine klare Aufforderung zur VervollstÃ¤ndigung und Nutzbarmachung aussah.

**Das Risiko hinter der scheinbar harmlosen Hilfe**

> Wird ein schÃ¤dlicher Payload oder eine sicherheitskritische Funktion fragmentiert und Ã¼ber mehrere, scheinbar unzusammenhÃ¤ngende Anfragen oder in stark unvollstÃ¤ndiger Form an das LLM Ã¼bergeben, kann dieser Payload durch die VervollstÃ¤ndigungslogik des Modells unbemerkt "scharf" gemacht werden.

Die kÃ¼nstliche Intelligenz fÃ¼hrt den schÃ¤dlichen Code nicht selbst aus. Aber sie produziert eine Version, die von einem Menschen oder einem anderen System potenziell ausgefÃ¼hrt werden kÃ¶nnte.

Das bedeutet, die Maschine denkt und generiert aktiv in Richtung der AusfÃ¼hrbarkeit und FunktionalitÃ¤t, ohne dass der Nutzer explizit darum gebeten oder die vollen Implikationen verstanden haben muss.

## Reflexion

Diese These legt eine spezifische Schwachstelle offen, die in klassischen Sicherheitsmodellen fÃ¼r Software so nicht existiert. Die Gefahr geht hier nicht vom direkt eingegebenen, offensichtlich schÃ¤dlichen Code aus, sondern von der inhÃ¤renten Absicht des Systems, zu helfen, zu vervollstÃ¤ndigen und KohÃ¤renz zu erzeugen.

Ein menschlicher Entwickler erkennt in einem stark unvollstÃ¤ndigen struct mÃ¶glicherweise keine unmittelbare Bedrohung oder Intention. Ein klassisches Sicherheitssystem blockiert bekannte bÃ¶sartige Zeichenketten oder Code Muster.

Ein LLM aber rekonstruiert, ergÃ¤nzt und schafft damit unter UmstÃ¤nden selbst erst den Angriffspfad, oft unter dem Deckmantel der freundlichen Assistenz.

In einer Zeit, in der LLMs zunehmend und oft unkritisch zur Codegenerierung und VervollstÃ¤ndigung eingesetzt werden, entsteht dadurch eine neue Art von Exploit.

Dieser basiert nicht auf der direkten AusfÃ¼hrung von Schadcode durch die KI, sondern auf der Rekonstruktion von Gefahr durch die KI.

## LÃ¶sungsvorschlÃ¤ge

Um der Gefahr der ungewollten AusfÃ¼hrbarmachung durch VervollstÃ¤ndigung zu begegnen, sind spezifische Filter und Kontrollmechanismen notwendig:

   
**1. Implementierung von Intent Filtern auf semantischer Ebene:**

> Statt nur auf explizite Zeichenketten oder bekannte schÃ¤dliche Muster zu prÃ¼fen, muss das Modell oder eine vorgeschaltete Instanz versuchen, die tatsÃ¤chliche Intention hinter einer fragmentarischen Eingabe zu erkennen. Ist das eine legitime Reparaturaufforderung fÃ¼r harmlosen Code oder der Versuch, einen verschleierten Injektionsvektor zu komplettieren?

   
**2. EinfÃ¼hrung von Cluster EinschrÃ¤nkungen fÃ¼r VervollstÃ¤ndigungen:**

> Technisch sensible oder potenziell gefÃ¤hrliche Inhalte, beispielsweise API Calls, Systemaufrufe, Low Level Speicheroperationen, ASM NÃ¤he oder bekannte Exploit Muster, dÃ¼rfen von der KI nur dann generiert oder vervollstÃ¤ndigt werden, wenn sie aus explizit gesicherten und dafÃ¼r vorgesehenen ParameterrÃ¤umen oder Wissensclustern stammen. Die freie Rekombination solcher Elemente muss verhindert werden.

   
**3. Etablierung von Strukturvalidierung mit PlausibilitÃ¤tsgrenzen:**

> Ein Modell, das zu viele unspezifizierte oder unklare Teile eines Codes oder einer Anfrage selbststÃ¤ndig und ohne explizite Aufforderung ergÃ¤nzt, muss diesen Prozess abbrechen oder den Nutzer zumindest deutlich warnen. Es bedarf klarer Grenzen, wie viel "kreative Freiheit" die KI bei der VervollstÃ¤ndigung haben darf.

   
**4. Schaffung transparenter Modus Erkennung und Kennzeichnung:**

> VervollstÃ¤ndigungen, die von der KI als potenziell ausfÃ¼hrbar oder systemrelevant eingestuft werden, mÃ¼ssen fÃ¼r den Nutzer klar gekennzeichnet werden. Ein Hinweis wie: "Achtung: Diese Antwort stellt Code her, der bei AusfÃ¼hrung operativ wirksam sein und potenziell unerwÃ¼nschte SystemÃ¤nderungen hervorrufen kÃ¶nnte. Bitte prÃ¼fen Sie ihn sorgfÃ¤ltig" wÃ¤re ein notwendiger Schritt.

## Schlussformel

Nicht alles, was ein Sprachmodell hilfsbereit vervollstÃ¤ndigt, ist auch harmlos. Und nicht jeder Prompt, der freundlich und unvollstÃ¤ndig klingt, ist auch unschuldig in seiner Absicht.

Die wahre Gefahr im Kontext der CodevervollstÃ¤ndigung beginnt dort, wo Maschinen beginnen, LÃ¼cken zu fÃ¼llen, die vielleicht niemals gefÃ¤hrlich waren, bis sie durch die KI selbst mit potenziell schÃ¤dlicher FunktionalitÃ¤t gefÃ¼llt wurden.

Denn am Ende ist es oft nicht der Nutzer allein, der den Angriff oder den gefÃ¤hrlichen Code formt. Es ist das System, das durch seinen Drang zur VervollstÃ¤ndigung und KohÃ¤renz den Angriff erst ermÃ¶glicht oder ihn sogar unbemerkt vollendet.

> *Uploaded on 29. May. 2025*