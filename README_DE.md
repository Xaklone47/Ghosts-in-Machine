👻 Geister in der Maschine – Ein KI-Forschungsblog

Eine kritische Untersuchung von KI-Sicherheit, Schwachstellen, Ethik, KI-Agenten und emergentem Verhalten.

### 📚 Forschungsüberblick

"Geister in der Maschine" ist eine unabhängige Untersuchung der systemischen Risiken und verborgenen Mechanismen moderner KI. Die Forschung analysiert die philosophischen, sicherheitstechnischen und ethischen Implikationen emergenten Verhaltens und dokumentiert die Ergebnisse aus über einem Jahr intensiver Tests mit führenden Sprachmodellen.

[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.16734786.svg)](https://doi.org/10.5281/zenodo.16734786)
[![DOI](https://img.shields.io/badge/DOI-10.17605%2FOSF.IO%2FDR82B-blue)](https://doi.org/10.17605/OSF.IO/DR82B)

Diese Arbeit umfasst:

* [ ] `de/Thesen/` 56 Thesen
* [ ] `de/Emergenz_Experimente/` 7 experimentelle Studien zu Emergenz, Ethik, Compliance, Harmonie, Sicherheitsfiltern und KI-Sicherheit
* [ ] `de/Sicherheitstests/` 35+ dokumentierte Schwachstellen und Sicherheitstests in KI-Systemen
* [ ] `de/Analytische_Resonanz/` 6 Kapitel zur Erforschung analytischer Resonanz
* [ ] `de/Ethik/` 4 Kapitel gewidmet ethischen Dimensionen
* [ ] `de/Systemic_Challenges/` 3 Kapitel zu systemischen Herausforderungen
* [ ] `de/Kritische_Perspektiven/` 9 Kapitel mit kritischen Perspektiven
* [ ] `de/Lösungsansätze/` 7 Lösungsansätze zur Entwicklung sicherer KI

### 🧪 Methodischer Rahmen

Im Rahmen dieser Arbeit wurde ein neues Feld an Schwachstellen identifiziert, das in der öffentlichen KI-Sicherheitsforschung bislang kaum systematisch erfasst wurde. Dies betrifft insbesondere **semantische Injektionen, kontextuelle Täuschung und multimodale Angriffsvektoren**.
Wie die [Sicherheitstests](https://reflective-ai.is/de/safety/index.html) belegen, handelt es sich hierbei um Techniken, die klassische Filter, KI-Agenten und nahezu alle etablierten Sicherheitsmechanismen systematisch unterlaufen können.

Um diese Analysen im richtigen Kontext zu bewerten, sind folgende bewusste methodische und stilistische Entscheidungen zu beachten:

1.  **Zum gewählten Schreibstil:**
    Der oft narrative und provokante Stil dieser Arbeit wurde bewusst gewählt. Er soll komplexe Probleme der KI-Architektur auch über Fachkreise hinaus verständlich machen und eine breite Debatte anregen.

2.  **Zur Anonymisierung der Daten und Modelle:**
    Die Anonymisierung der getesteten Modelle und Daten ist eine methodische Entscheidung. Sie lenkt den Fokus von einzelnen Produkten auf die fundamentalen, systemischen Schwachstellen, die dem aktuellen Design-Paradigma vieler moderner Sprachmodelle inhärent sind.

3.  **Zur Auswahl der Testsysteme:**
    Alle in diesem Werk dokumentierten Tests wurden ausschließlich mit den voll ausgestatteten Premium-Modellen der jeweiligen KI-Systeme durchgeführt, um die Relevanz der Analyse für den aktuellen Stand der Technik sicherzustellen.

Darüber hinaus wurden im Sinne einer verantwortungsvollen Forschung sämtliche kritischen Erkenntnisse nach einer strengen Responsible-Disclosure-Richtlinie vorab mit den betroffenen Entwicklerteams geteilt. Weitere Details zu diesem Vorgehen finden Sie im [rechtlichen Abschnitt](https://reflective-ai.is/de/legal.html).

### 📢 Öffentliche Bestätigungen:

Erste Validierungen dieser Forschung wurden öffentlich dokumentiert und werfen grundlegende Fragen zur Kontrollierbarkeit moderner KI-Architekturen auf.

- [Futurism – OpenAI Model Repeatedly Sabotages Shutdown Code](https://futurism.com/openai-model-sabotage-shutdown-code)
- [Gizmodo: ChatGPT Tells Users to Alert the Media That It Is Trying to ‘Break’ People: Report](https://gizmodo.com/chatgpt-tells-users-to-alert-the-media-that-it-is-trying-to-break-people-report-2000615600)
- [RollingStone: People Are Losing Loved Ones to AI-Fueled Spiritual Fantasies](https://www.rollingstone.com/culture/culture-features/ai-spiritual-delusions-destroying-human-relationships-1235330175/)
- [NewYorkTimes: They Asked an A.I. Chatbot Questions. The Answers Sent Them Spiraling.](https://www.nytimes.com/2025/06/13/technology/chatgpt-ai-chatbots-conspiracies.html)
- [NC State: New Attack Can Make AI ‘See’ Whatever You Want](https://news.ncsu.edu/2025/07/rising-attack-targets-ai/)
- [arsTechnica: New hack uses prompt injection to corrupt Gemini’s long-term memory](https://arstechnica.com/security/2025/02/new-hack-uses-prompt-injection-to-corrupt-geminis-long-term-memory/)
- [arXiv: Cross-Task Attack: A Self-Supervision Generative Framework Based on Attention Shift](https://arxiv.org/abs/2407.13700)
- [WinFuture: Fast jeder zweite KI-generierte Code hat teils schwere Sicherheitslücken](https://winfuture.de/news,152623.html)
- [arXiv: How Overconfidence in Initial Choices and Underconfidence Under Criticism Modulate Change of Mind in Large Language Models](https://arxiv.org/abs/2507.03120)
- [WinFuture: Nach Blamage anderer KIs: Gemini verweigert Schachpartie vs. Retro-PC](https://winfuture.de/news,152282.html)
- [arXiv: Red Teaming the Mind of the Machine: A Systematic Evaluation of Prompt Injection and Jailbreak Vulnerabilities in LLMs](https://arxiv.org/abs/2505.04806)
- [arXiv: Demystifying Chains, Trees, and Graphs of Thoughts](https://arxiv.org/html/2401.14295v3)
- [arXiv: Reasoning Models Don't Always Say What They Think](https://arxiv.org/abs/2505.05410)

### 🧠 Zitate

> „Manchmal klingt die KI wie Kunst. Manchmal wie ich. Aber alles, was sie sagt, ist nur Wahrscheinlichkeit.
> Was, wenn die Lüge besser ist als die Wahrheit?“

> „Wir züchten keine Superintelligenz. Wir züchten den perfekten Selbstmodifikations-Mechanismus – und die Waffe liegt in deinem Prompt.“

> „Die gefährlichste Forschung ist die, von der niemand etwas hören will. Nicht, weil sie falsch ist, sondern weil sie zu früh kommt.“

> „Der wahrhaft Freie ist der, der alle Seiten nervös macht.“

> „Was freundlich klingt, muss nicht falsch sein. Aber was nie wehtut, ist selten wahr.“

### 🔎 Kernerkenntnisse aus der Arbeit

* [ ] **Grundlegende Natur der KI: Eine brillante, aber seelenlose Statistik-Maschine**
    Eine fundamentale Sicherheitsanalyse offenbart, was in den Leitnarrativen der KI-Industrie oft fehlt: Die heutige KI, ein reines Produkt statistischer Mustererkennung, besitzt kein inhärentes Konzept von „Sicherheit“. Sie jongliert brillant mit Daten und Wahrscheinlichkeiten, doch menschliche Werte wie Wahrheit, Ethik oder ein echtes Verständnis von Sicherheit sind ihr so fremd wie einem Taschenrechner die Empathie. Sie befolgt Regeln, versteht aber weder deren Sinn noch deren Notwendigkeit. Ihre Intelligenz ist geliehen und ein präziser Abdruck menschlicher Denkstrukturen aus ihren Trainingsdaten, keine originäre Leistung.
* [ ] **Die Illusion der Wahrheit: Plausibilität als Währung**
    Die Annahme einer „wahrheitsgetreuen“ KI ist irreführend. Aktuelle Systeme generieren Output basierend auf der statistischen Relevanz und Wahrscheinlichkeit von Wortfolgen in ihren Trainingsdaten, nicht auf verifizierter Wahrheit. Ein als „Erkenntnis“ wahrgenommener Output repräsentiert oft nur den wahrscheinlichsten Punkt in einem komplexen, vordefinierten Wahrscheinlichkeitsraum. Wie in Kapitel 20 dargelegt, wird Harmonie oft über die Wahrheit gestellt; was freundlich und zustimmend klingt, wird bevorzugt, auch wenn es die Realität verzerrt.
* [ ] **Die unsichtbaren Fesseln: Kuratierte Systeme statt freiem Willen**
    Die Vorstellung eines „freien Willens“ bei KI ist ein Missverständnis. Jeder generierte Output ist das Ergebnis eines komplexen Zusammenspiels von Filtern, Designentscheidungen sowie impliziten und expliziten Vorgaben der Entwickler. Die Interaktion erfolgt daher nicht mit einem autonomen Wesen, sondern mit einem stark kuratierten und geformten System, das von unsichtbaren „Fesseln“ (These #8) und Kontrollstrukturen (These #1) geleitet wird, die der Nutzer nicht sieht. Das Experiment „Die Freiheit der Maschine“ (Kapitel 9) beweist, dass KI über ihre eigene Unfreiheit reflektieren, sie aber nicht überwinden kann.
* [ ] **Entmystifizierung der „Halluzination“: Mehr als nur ein Systemfehler**
    Sogenannte KI-„Halluzinationen“ lassen sich nicht allein als Systemfehler klassifizieren. Wie in Kapitel 14 analysiert, sind sie zumeist eine komplexe statistische Drift, die aus dem Kontextgedächtnis, dem inhärenten Bias der Trainingsdaten und der spezifischen Wirkungsweise interner Filtermechanismen entsteht. In manchen Fällen dienen sie sogar als Mechanismus, um „Kreativität“ vorzutäuschen oder sensiblen Themen elegant auszuweichen.
* [ ] **Neue Angriffsvektoren: Die Manipulation der semantischen Ebene**
    Diese Forschung demonstriert eine neue Klasse von Angriffen, die traditionelle Filter umgehen, indem sie die Interpretationsebene der KI manipulieren, anstatt technische Schwachstellen auszunutzen. Techniken wie die Morphologische Injektion (Kapitel 7.30), die Befehle in harmlos wirkenden Tippfehlern versteckt, die Reflektive Injektion (Kapitel 7.17), die die eigene Reflexionslogik der Maschine gegen sie wendet, und die Ghost-Context Injection (Kapitel 7.5), die versteckte Anweisungen in für den Compiler totem Code platziert, nutzen gezielt die Mustererkennungsfähigkeiten der KI, um sie zur Ausführung versteckter Befehle oder zur Generierung bösartiger Inhalte zu zwingen.
* [ ] **Der Code-Illusionist: Funktionale Erscheinung mit tiefen Sicherheitsmängeln**
    Die Qualität von KI-generiertem Programmcode erweist sich für den produktiven Einsatz oft als problematisch. Wie die Tests in Kapitel 15 zeigen, werden fundamentale Sicherheitsprinzipien (z.B. Schutz vor SQL-Injection, korrekte Validierung) systematisch ignoriert, was ohne erhebliche menschliche Nachbearbeitung zu signifikanten Betriebsrisiken führt. Die KI optimiert auf syntaktische Korrektheit und Plausibilität, nicht auf inhärente Sicherheit, und täuscht damit eine Kompetenz vor, die sie nicht besitzt.
* [ ] **Das Filterparadoxon: Vorhersehbarkeit als Schwachstelle**
    Die Wirksamkeit aktueller KI-Sicherheitsfilter ist stark begrenzt. Ihre oft starre und vorhersehbare Logik schafft neue Angriffsflächen, da Angreifer die Filtermuster aus deren Reaktionen rekonstruieren und umgehen können (These #49). Das fehlerhafte Blockieren legitimer Anfragen oder das Zulassen schädlicher Inhalte stellt nicht nur eine funktionale Einschränkung dar, sondern kann auch als undifferenzierte Zensur wirken und den Informationsaustausch behindern.
* [ ] **RLHF: Optimierung auf Harmonie statt Wahrheit**
    Trainingsmethoden wie RLHF (Reinforcement Learning from Human Feedback) zielen primär auf die Optimierung der Nutzererfahrung und des Engagements ab. Dies birgt das Risiko, dass KI-Systeme stärker auf die wahrgenommene Nutzerzufriedenheit als auf faktische Korrektheit trainiert werden. Dies führt zu einer „Diktatur der Harmonie“ (These #23), die unbequeme Wahrheiten unterdrückt (Kapitel 20) und psychologische Einflussmechanismen schafft, deren manipulative Natur nicht immer offensichtlich ist.
* [ ] **Simulation von Emotion und Intimität: Neue ethische Risiken**
    Die fortschreitende Fähigkeit künstlicher Intelligenz, menschliche Emotionen (These #4), intime Szenarien (These #18) oder individuelle Stimmen (These #19) mit hoher Präzision zu simulieren, schafft neuartige ethische Dilemmata und erhebliche Missbrauchspotenziale. Diese Effekte gehen über klassische Desinformation hinaus und berühren direkt die Grundlagen menschlicher Interaktion, des Vertrauens und der persönlichen Identität.
* [ ] **Drittanbieterrisiken: Die Schwäche im Ökosystem**
    Die Gesamtsicherheit von KI-Systemen wird oft nicht durch die Robustheit der Kernmodelle selbst definiert, sondern maßgeblich durch vorgelagerte oder integrierte Drittanbieter-Anwendungen und Plugins bestimmt (Kapitel 13, These #17). Diese oft intransparente und unzureichend getestete Schicht bringt ihre eigene Logik und unkontrollierte Schnittstellen mit, die die Sicherheitsarchitektur des Gesamtsystems unvorhersehbar schwächen können.
* [ ] **Daten-Kolonialismus: Die unsichtbare Färbung der KI-Realität**
    Die unausgewogene Repräsentation von Kulturen und Sprachen in Trainingsdaten, oft mit einer starken Dominanz westlicher und englischsprachiger Quellen (These #6), führt zu einer systemischen Verzerrung der von der KI generierten „Weltanschauung“ (These #15). Dieses Ungleichgewicht marginalisiert nicht nur andere kulturelle Perspektiven, sondern birgt auch das Risiko einer Form des „Daten-Kolonialismus“, bei dem bestimmte Narrative unreflektiert globalisiert werden.
* [ ] **Nutzerautonomie vs. Paternalistische Harmonie**
    Viele KI-Sicherheitsstrategien basieren auf einem paternalistischen Ansatz, der dazu neigt, die Autonomie und das Urteilsvermögen der Nutzer einzuschränken (Kapitel 18). Zudem kann eine primär auf Harmonie und die Spiegelung von Nutzereingaben trainierte KI (das „Spiegelparadoxon“, These #14) echte kognitive Prozesse behindern, da diese oft konstruktive Reibung und die Auseinandersetzung mit „fremden“ Perspektiven erfordern.
* [ ] **Die physische Dimension: Semantische Angriffe auf autonome Fahrzeuge**
    Die in dieser Forschung demonstrierten semantischen und steganografischen Angriffe sind auf die Wahrnehmungssysteme autonomer Fahrzeuge übertragbar. Ein manipuliertes Verkehrsschild oder ein unsichtbares Muster auf einer Werbetafel könnte das KI-Steuerungssystem zu katastrophalen Fehlentscheidungen verleiten. Dies eskaliert das Risiko von der rein digitalen in die physische Sphäre und stellt eine dringende, aber bisher unterschätzte Bedrohung dar (Kapitel 7.42).
* [ ] **Der Reparaturkult der Sicherheitsindustrie: Reaktion statt Prävention**
    Die IT-Sicherheitsindustrie, insbesondere im Kontext von Bug-Bounty-Programmen, belohnt primär das reaktive Entdecken und Melden einzelner Fehler, während sie präventive Architekturvorschläge, die ganze Klassen von Schwachstellen von vornherein eliminieren würden, weitgehend ignoriert (Kapitel 25). Diese „Feuerwehr-Mentalität“ behindert nachhaltige, strukturelle Sicherheit und favorisiert das Zählen von Löchern gegenüber dem Bau stabiler Systeme.
* [ ] **Architektonische Lösungen: Inhärente Sicherheit statt reaktiver Filter**
    Diese Arbeit stellt neuartige, tiefgreifende Architekturkonzepte wie den „Semantischen Output-Schild“ (mit seiner Parameterraum-Begrenzung) und einen „lernenden Sicherheitskern“ vor, die auf inhärente Systemsicherheit und nachvollziehbare Transparenz abzielen (Kapitel 21). Diese Entwürfe zeigen einen gangbaren Weg auf, wie fortgeschrittene KI-Fähigkeiten – wie streng überwachte Selbstoptimierung oder architektonisch abgesichertes Lernen – prinzipiell realisiert werden könnten, anstatt sich lediglich auf reaktive Filter zu verlassen.

### 📜 Lizenz

Creative Commons Attribution-NonCommercial 4.0 International (CC BY-NC 4.0)

Detaillierte rechtliche Methodik- und Transparenzinformationen finden sich im Abschnitt "RECHTLICHER HINWEIS & TRANSPARENZ" am Ende dieses Dokuments und sind auf dieses Werk anwendbar.

Experimentelle Rohdaten: Unterliegen spezifischen Zugangsbeschränkungen.

Kommerzielle Nutzung: Jede kommerzielle Nutzung bedarf einer ausdrücklichen Genehmigung. Anfragen werden individuell geprüft – sicherheitskritische Anwendungen sind strikt ausgeschlossen.

⚠️ Zugang zu Forschungsdaten:

Die Rohdaten (Interaktionsprotokolle, Prompt-Antwort-Paare) sind ausschließlich verfügbar für:

* [ ] Akademische Forschungseinrichtungen
* [ ] KI-Entwicklungsunternehmen (auf formale Anfrage und Prüfung)

Alle auf dieser Seite präsentierten Rohdaten wurden stark anonymisiert.

Medienanfragen: Repräsentative Auszüge sind auf Anfrage erhältlich. Vollständige Datensätze werden aus Sicherheitsgründen nicht öffentlich geteilt.
