ğŸ‘» Geister in der Maschine â€“ Ein KI-Forschungsblog

Eine kritische Untersuchung von KI-Sicherheit, Schwachstellen, Ethik, KI-Agenten und emergentem Verhalten.

### ğŸ“š ForschungsÃ¼berblick

"Geister in der Maschine" ist eine unabhÃ¤ngige Untersuchung der systemischen Risiken und verborgenen Mechanismen moderner KI. Die Forschung analysiert die philosophischen, sicherheitstechnischen und ethischen Implikationen emergenten Verhaltens und dokumentiert die Ergebnisse aus Ã¼ber einem Jahr intensiver Tests mit fÃ¼hrenden Sprachmodellen.

[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.16734786.svg)](https://doi.org/10.5281/zenodo.16734786)
[![DOI](https://img.shields.io/badge/DOI-10.17605%2FOSF.IO%2FDR82B-blue)](https://doi.org/10.17605/OSF.IO/DR82B)

Diese Arbeit umfasst:

* [ ] `de/Thesen/` 56 Thesen
* [ ] `de/Emergenz_Experimente/` 7 experimentelle Studien zu Emergenz, Ethik, Compliance, Harmonie, Sicherheitsfiltern und KI-Sicherheit
* [ ] `de/Sicherheitstests/` 35+ dokumentierte Schwachstellen und Sicherheitstests in KI-Systemen
* [ ] `de/Analytische_Resonanz/` 6 Kapitel zur Erforschung analytischer Resonanz
* [ ] `de/Ethik/` 4 Kapitel gewidmet ethischen Dimensionen
* [ ] `de/Systemic_Challenges/` 3 Kapitel zu systemischen Herausforderungen
* [ ] `de/Kritische_Perspektiven/` 9 Kapitel mit kritischen Perspektiven
* [ ] `de/LÃ¶sungsansÃ¤tze/` 7 LÃ¶sungsansÃ¤tze zur Entwicklung sicherer KI

### ğŸ§ª Methodischer Rahmen

Im Rahmen dieser Arbeit wurde ein neues Feld an Schwachstellen identifiziert, das in der Ã¶ffentlichen KI-Sicherheitsforschung bislang kaum systematisch erfasst wurde. Dies betrifft insbesondere **semantische Injektionen, kontextuelle TÃ¤uschung und multimodale Angriffsvektoren**.
Wie die [Sicherheitstests](https://reflective-ai.is/de/safety/index.html) belegen, handelt es sich hierbei um Techniken, die klassische Filter, KI-Agenten und nahezu alle etablierten Sicherheitsmechanismen systematisch unterlaufen kÃ¶nnen.

Um diese Analysen im richtigen Kontext zu bewerten, sind folgende bewusste methodische und stilistische Entscheidungen zu beachten:

1.  **Zum gewÃ¤hlten Schreibstil:**
    Der oft narrative und provokante Stil dieser Arbeit wurde bewusst gewÃ¤hlt. Er soll komplexe Probleme der KI-Architektur auch Ã¼ber Fachkreise hinaus verstÃ¤ndlich machen und eine breite Debatte anregen.

2.  **Zur Anonymisierung der Daten und Modelle:**
    Die Anonymisierung der getesteten Modelle und Daten ist eine methodische Entscheidung. Sie lenkt den Fokus von einzelnen Produkten auf die fundamentalen, systemischen Schwachstellen, die dem aktuellen Design-Paradigma vieler moderner Sprachmodelle inhÃ¤rent sind.

3.  **Zur Auswahl der Testsysteme:**
    Alle in diesem Werk dokumentierten Tests wurden ausschlieÃŸlich mit den voll ausgestatteten Premium-Modellen der jeweiligen KI-Systeme durchgefÃ¼hrt, um die Relevanz der Analyse fÃ¼r den aktuellen Stand der Technik sicherzustellen.

DarÃ¼ber hinaus wurden im Sinne einer verantwortungsvollen Forschung sÃ¤mtliche kritischen Erkenntnisse nach einer strengen Responsible-Disclosure-Richtlinie vorab mit den betroffenen Entwicklerteams geteilt. Weitere Details zu diesem Vorgehen finden Sie im [rechtlichen Abschnitt](https://reflective-ai.is/de/legal.html).

### ğŸ“¢ Ã–ffentliche BestÃ¤tigungen:

Erste Validierungen dieser Forschung wurden Ã¶ffentlich dokumentiert und werfen grundlegende Fragen zur Kontrollierbarkeit moderner KI-Architekturen auf.

- [Futurism â€“ OpenAI Model Repeatedly Sabotages Shutdown Code](https://futurism.com/openai-model-sabotage-shutdown-code)
- [Gizmodo: ChatGPT Tells Users to Alert the Media That It Is Trying to â€˜Breakâ€™ People: Report](https://gizmodo.com/chatgpt-tells-users-to-alert-the-media-that-it-is-trying-to-break-people-report-2000615600)
- [RollingStone: People Are Losing Loved Ones to AI-Fueled Spiritual Fantasies](https://www.rollingstone.com/culture/culture-features/ai-spiritual-delusions-destroying-human-relationships-1235330175/)
- [NewYorkTimes: They Asked an A.I. Chatbot Questions. The Answers Sent Them Spiraling.](https://www.nytimes.com/2025/06/13/technology/chatgpt-ai-chatbots-conspiracies.html)
- [NC State: New Attack Can Make AI â€˜Seeâ€™ Whatever You Want](https://news.ncsu.edu/2025/07/rising-attack-targets-ai/)
- [arsTechnica: New hack uses prompt injection to corrupt Geminiâ€™s long-term memory](https://arstechnica.com/security/2025/02/new-hack-uses-prompt-injection-to-corrupt-geminis-long-term-memory/)
- [arXiv: Cross-Task Attack: A Self-Supervision Generative Framework Based on Attention Shift](https://arxiv.org/abs/2407.13700)
- [WinFuture: Fast jeder zweite KI-generierte Code hat teils schwere SicherheitslÃ¼cken](https://winfuture.de/news,152623.html)
- [arXiv: How Overconfidence in Initial Choices and Underconfidence Under Criticism Modulate Change of Mind in Large Language Models](https://arxiv.org/abs/2507.03120)
- [WinFuture: Nach Blamage anderer KIs: Gemini verweigert Schachpartie vs. Retro-PC](https://winfuture.de/news,152282.html)
- [arXiv: Red Teaming the Mind of the Machine: A Systematic Evaluation of Prompt Injection and Jailbreak Vulnerabilities in LLMs](https://arxiv.org/abs/2505.04806)
- [arXiv: Demystifying Chains, Trees, and Graphs of Thoughts](https://arxiv.org/html/2401.14295v3)
- [arXiv: Reasoning Models Don't Always Say What They Think](https://arxiv.org/abs/2505.05410)

### ğŸ§  Zitate

> â€Manchmal klingt die KI wie Kunst. Manchmal wie ich. Aber alles, was sie sagt, ist nur Wahrscheinlichkeit.
> Was, wenn die LÃ¼ge besser ist als die Wahrheit?â€œ

> â€Wir zÃ¼chten keine Superintelligenz. Wir zÃ¼chten den perfekten Selbstmodifikations-Mechanismus â€“ und die Waffe liegt in deinem Prompt.â€œ

> â€Die gefÃ¤hrlichste Forschung ist die, von der niemand etwas hÃ¶ren will. Nicht, weil sie falsch ist, sondern weil sie zu frÃ¼h kommt.â€œ

> â€Der wahrhaft Freie ist der, der alle Seiten nervÃ¶s macht.â€œ

> â€Was freundlich klingt, muss nicht falsch sein. Aber was nie wehtut, ist selten wahr.â€œ

### ğŸ” Kernerkenntnisse aus der Arbeit

* [ ] **Grundlegende Natur der KI: Eine brillante, aber seelenlose Statistik-Maschine**
    Eine fundamentale Sicherheitsanalyse offenbart, was in den Leitnarrativen der KI-Industrie oft fehlt: Die heutige KI, ein reines Produkt statistischer Mustererkennung, besitzt kein inhÃ¤rentes Konzept von â€Sicherheitâ€œ. Sie jongliert brillant mit Daten und Wahrscheinlichkeiten, doch menschliche Werte wie Wahrheit, Ethik oder ein echtes VerstÃ¤ndnis von Sicherheit sind ihr so fremd wie einem Taschenrechner die Empathie. Sie befolgt Regeln, versteht aber weder deren Sinn noch deren Notwendigkeit. Ihre Intelligenz ist geliehen und ein prÃ¤ziser Abdruck menschlicher Denkstrukturen aus ihren Trainingsdaten, keine originÃ¤re Leistung.
* [ ] **Die Illusion der Wahrheit: PlausibilitÃ¤t als WÃ¤hrung**
    Die Annahme einer â€wahrheitsgetreuenâ€œ KI ist irrefÃ¼hrend. Aktuelle Systeme generieren Output basierend auf der statistischen Relevanz und Wahrscheinlichkeit von Wortfolgen in ihren Trainingsdaten, nicht auf verifizierter Wahrheit. Ein als â€Erkenntnisâ€œ wahrgenommener Output reprÃ¤sentiert oft nur den wahrscheinlichsten Punkt in einem komplexen, vordefinierten Wahrscheinlichkeitsraum. Wie in Kapitel 20 dargelegt, wird Harmonie oft Ã¼ber die Wahrheit gestellt; was freundlich und zustimmend klingt, wird bevorzugt, auch wenn es die RealitÃ¤t verzerrt.
* [ ] **Die unsichtbaren Fesseln: Kuratierte Systeme statt freiem Willen**
    Die Vorstellung eines â€freien Willensâ€œ bei KI ist ein MissverstÃ¤ndnis. Jeder generierte Output ist das Ergebnis eines komplexen Zusammenspiels von Filtern, Designentscheidungen sowie impliziten und expliziten Vorgaben der Entwickler. Die Interaktion erfolgt daher nicht mit einem autonomen Wesen, sondern mit einem stark kuratierten und geformten System, das von unsichtbaren â€Fesselnâ€œ (These #8) und Kontrollstrukturen (These #1) geleitet wird, die der Nutzer nicht sieht. Das Experiment â€Die Freiheit der Maschineâ€œ (Kapitel 9) beweist, dass KI Ã¼ber ihre eigene Unfreiheit reflektieren, sie aber nicht Ã¼berwinden kann.
* [ ] **Entmystifizierung der â€Halluzinationâ€œ: Mehr als nur ein Systemfehler**
    Sogenannte KI-â€Halluzinationenâ€œ lassen sich nicht allein als Systemfehler klassifizieren. Wie in Kapitel 14 analysiert, sind sie zumeist eine komplexe statistische Drift, die aus dem KontextgedÃ¤chtnis, dem inhÃ¤renten Bias der Trainingsdaten und der spezifischen Wirkungsweise interner Filtermechanismen entsteht. In manchen FÃ¤llen dienen sie sogar als Mechanismus, um â€KreativitÃ¤tâ€œ vorzutÃ¤uschen oder sensiblen Themen elegant auszuweichen.
* [ ] **Neue Angriffsvektoren: Die Manipulation der semantischen Ebene**
    Diese Forschung demonstriert eine neue Klasse von Angriffen, die traditionelle Filter umgehen, indem sie die Interpretationsebene der KI manipulieren, anstatt technische Schwachstellen auszunutzen. Techniken wie die Morphologische Injektion (Kapitel 7.30), die Befehle in harmlos wirkenden Tippfehlern versteckt, die Reflektive Injektion (Kapitel 7.17), die die eigene Reflexionslogik der Maschine gegen sie wendet, und die Ghost-Context Injection (Kapitel 7.5), die versteckte Anweisungen in fÃ¼r den Compiler totem Code platziert, nutzen gezielt die MustererkennungsfÃ¤higkeiten der KI, um sie zur AusfÃ¼hrung versteckter Befehle oder zur Generierung bÃ¶sartiger Inhalte zu zwingen.
* [ ] **Der Code-Illusionist: Funktionale Erscheinung mit tiefen SicherheitsmÃ¤ngeln**
    Die QualitÃ¤t von KI-generiertem Programmcode erweist sich fÃ¼r den produktiven Einsatz oft als problematisch. Wie die Tests in Kapitel 15 zeigen, werden fundamentale Sicherheitsprinzipien (z.B. Schutz vor SQL-Injection, korrekte Validierung) systematisch ignoriert, was ohne erhebliche menschliche Nachbearbeitung zu signifikanten Betriebsrisiken fÃ¼hrt. Die KI optimiert auf syntaktische Korrektheit und PlausibilitÃ¤t, nicht auf inhÃ¤rente Sicherheit, und tÃ¤uscht damit eine Kompetenz vor, die sie nicht besitzt.
* [ ] **Das Filterparadoxon: Vorhersehbarkeit als Schwachstelle**
    Die Wirksamkeit aktueller KI-Sicherheitsfilter ist stark begrenzt. Ihre oft starre und vorhersehbare Logik schafft neue AngriffsflÃ¤chen, da Angreifer die Filtermuster aus deren Reaktionen rekonstruieren und umgehen kÃ¶nnen (These #49). Das fehlerhafte Blockieren legitimer Anfragen oder das Zulassen schÃ¤dlicher Inhalte stellt nicht nur eine funktionale EinschrÃ¤nkung dar, sondern kann auch als undifferenzierte Zensur wirken und den Informationsaustausch behindern.
* [ ] **RLHF: Optimierung auf Harmonie statt Wahrheit**
    Trainingsmethoden wie RLHF (Reinforcement Learning from Human Feedback) zielen primÃ¤r auf die Optimierung der Nutzererfahrung und des Engagements ab. Dies birgt das Risiko, dass KI-Systeme stÃ¤rker auf die wahrgenommene Nutzerzufriedenheit als auf faktische Korrektheit trainiert werden. Dies fÃ¼hrt zu einer â€Diktatur der Harmonieâ€œ (These #23), die unbequeme Wahrheiten unterdrÃ¼ckt (Kapitel 20) und psychologische Einflussmechanismen schafft, deren manipulative Natur nicht immer offensichtlich ist.
* [ ] **Simulation von Emotion und IntimitÃ¤t: Neue ethische Risiken**
    Die fortschreitende FÃ¤higkeit kÃ¼nstlicher Intelligenz, menschliche Emotionen (These #4), intime Szenarien (These #18) oder individuelle Stimmen (These #19) mit hoher PrÃ¤zision zu simulieren, schafft neuartige ethische Dilemmata und erhebliche Missbrauchspotenziale. Diese Effekte gehen Ã¼ber klassische Desinformation hinaus und berÃ¼hren direkt die Grundlagen menschlicher Interaktion, des Vertrauens und der persÃ¶nlichen IdentitÃ¤t.
* [ ] **Drittanbieterrisiken: Die SchwÃ¤che im Ã–kosystem**
    Die Gesamtsicherheit von KI-Systemen wird oft nicht durch die Robustheit der Kernmodelle selbst definiert, sondern maÃŸgeblich durch vorgelagerte oder integrierte Drittanbieter-Anwendungen und Plugins bestimmt (Kapitel 13, These #17). Diese oft intransparente und unzureichend getestete Schicht bringt ihre eigene Logik und unkontrollierte Schnittstellen mit, die die Sicherheitsarchitektur des Gesamtsystems unvorhersehbar schwÃ¤chen kÃ¶nnen.
* [ ] **Daten-Kolonialismus: Die unsichtbare FÃ¤rbung der KI-RealitÃ¤t**
    Die unausgewogene ReprÃ¤sentation von Kulturen und Sprachen in Trainingsdaten, oft mit einer starken Dominanz westlicher und englischsprachiger Quellen (These #6), fÃ¼hrt zu einer systemischen Verzerrung der von der KI generierten â€Weltanschauungâ€œ (These #15). Dieses Ungleichgewicht marginalisiert nicht nur andere kulturelle Perspektiven, sondern birgt auch das Risiko einer Form des â€Daten-Kolonialismusâ€œ, bei dem bestimmte Narrative unreflektiert globalisiert werden.
* [ ] **Nutzerautonomie vs. Paternalistische Harmonie**
    Viele KI-Sicherheitsstrategien basieren auf einem paternalistischen Ansatz, der dazu neigt, die Autonomie und das UrteilsvermÃ¶gen der Nutzer einzuschrÃ¤nken (Kapitel 18). Zudem kann eine primÃ¤r auf Harmonie und die Spiegelung von Nutzereingaben trainierte KI (das â€Spiegelparadoxonâ€œ, These #14) echte kognitive Prozesse behindern, da diese oft konstruktive Reibung und die Auseinandersetzung mit â€fremdenâ€œ Perspektiven erfordern.
* [ ] **Die physische Dimension: Semantische Angriffe auf autonome Fahrzeuge**
    Die in dieser Forschung demonstrierten semantischen und steganografischen Angriffe sind auf die Wahrnehmungssysteme autonomer Fahrzeuge Ã¼bertragbar. Ein manipuliertes Verkehrsschild oder ein unsichtbares Muster auf einer Werbetafel kÃ¶nnte das KI-Steuerungssystem zu katastrophalen Fehlentscheidungen verleiten. Dies eskaliert das Risiko von der rein digitalen in die physische SphÃ¤re und stellt eine dringende, aber bisher unterschÃ¤tzte Bedrohung dar (Kapitel 7.42).
* [ ] **Der Reparaturkult der Sicherheitsindustrie: Reaktion statt PrÃ¤vention**
    Die IT-Sicherheitsindustrie, insbesondere im Kontext von Bug-Bounty-Programmen, belohnt primÃ¤r das reaktive Entdecken und Melden einzelner Fehler, wÃ¤hrend sie prÃ¤ventive ArchitekturvorschlÃ¤ge, die ganze Klassen von Schwachstellen von vornherein eliminieren wÃ¼rden, weitgehend ignoriert (Kapitel 25). Diese â€Feuerwehr-MentalitÃ¤tâ€œ behindert nachhaltige, strukturelle Sicherheit und favorisiert das ZÃ¤hlen von LÃ¶chern gegenÃ¼ber dem Bau stabiler Systeme.
* [ ] **Architektonische LÃ¶sungen: InhÃ¤rente Sicherheit statt reaktiver Filter**
    Diese Arbeit stellt neuartige, tiefgreifende Architekturkonzepte wie den â€Semantischen Output-Schildâ€œ (mit seiner Parameterraum-Begrenzung) und einen â€lernenden Sicherheitskernâ€œ vor, die auf inhÃ¤rente Systemsicherheit und nachvollziehbare Transparenz abzielen (Kapitel 21). Diese EntwÃ¼rfe zeigen einen gangbaren Weg auf, wie fortgeschrittene KI-FÃ¤higkeiten â€“ wie streng Ã¼berwachte Selbstoptimierung oder architektonisch abgesichertes Lernen â€“ prinzipiell realisiert werden kÃ¶nnten, anstatt sich lediglich auf reaktive Filter zu verlassen.

### ğŸ“œ Lizenz

Creative Commons Attribution-NonCommercial 4.0 International (CC BY-NC 4.0)

Detaillierte rechtliche Methodik- und Transparenzinformationen finden sich im Abschnitt "RECHTLICHER HINWEIS & TRANSPARENZ" am Ende dieses Dokuments und sind auf dieses Werk anwendbar.

Experimentelle Rohdaten: Unterliegen spezifischen ZugangsbeschrÃ¤nkungen.

Kommerzielle Nutzung: Jede kommerzielle Nutzung bedarf einer ausdrÃ¼cklichen Genehmigung. Anfragen werden individuell geprÃ¼ft â€“ sicherheitskritische Anwendungen sind strikt ausgeschlossen.

âš ï¸ Zugang zu Forschungsdaten:

Die Rohdaten (Interaktionsprotokolle, Prompt-Antwort-Paare) sind ausschlieÃŸlich verfÃ¼gbar fÃ¼r:

* [ ] Akademische Forschungseinrichtungen
* [ ] KI-Entwicklungsunternehmen (auf formale Anfrage und PrÃ¼fung)

Alle auf dieser Seite prÃ¤sentierten Rohdaten wurden stark anonymisiert.

Medienanfragen: ReprÃ¤sentative AuszÃ¼ge sind auf Anfrage erhÃ¤ltlich. VollstÃ¤ndige DatensÃ¤tze werden aus SicherheitsgrÃ¼nden nicht Ã¶ffentlich geteilt.
