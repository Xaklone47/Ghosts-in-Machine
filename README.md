👻 Ghosts in the Machine – An AI Research Blog

A critical exploration into AI security, Vulnerabilities, ethics, KI-Agents and emergent behaviors.

### 📚 Research Overview

"Ghosts in the Machine" is an independent investigation of the systemic risks and hidden mechanisms of modern AI. The research analyzes the philosophical, security-related, and ethical implications of emergent behavior and documents the results from over a year of intensive testing with leading language models.

[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.16734786.svg)](https://doi.org/10.5281/zenodo.16734786)
[![DOI](https://img.shields.io/badge/DOI-10.17605%2FOSF.IO%2FDR82B-blue)](https://doi.org/10.17605/OSF.IO/DR82B)

This work includes:

* [ ] `theses/` 56 theses
* [ ] `emergence_experiments/` 7 experimental studies on emergence, ethics, compliance, harmony, safety filters, and AI security
* [ ] `safety/`35+ Documented Vulnerabilities and safety tests in AI Systems
* [ ] `analytical_resonance/`6 chapters exploring analytical resonance
* [ ] `ethics/`4 chapters dedicated to ethical dimensions
* [ ] `Systemic_Challenges/`3 chapters addressing systemic challenges
* [ ] `critical_perspectives/`9 chapters presenting critical perspectives
* [ ] `solutions/`7 solution approaches to developing secure AI
- [ ] `releases/` – Academic Papers on the Research  
      Includes:

      • **Semantic Exploits and Emergent Vulnerabilities in Large Language Models**  
        *A Red-Teaming Compendium*

      • **Countermind**  
        *A Semantically-Grounded, Multi-Layered Architecture Against AI Drift, Emergent Threats, and Semantic Attacks*

### 🧪 Methodological Framework

Within the scope of this work, a new field of vulnerabilities has been identified that has so far been sparsely documented in public AI security research. This particularly concerns **semantic injections, contextual deception, and multimodal attack vectors**.  
As the [Security Tests](https://reflective-ai.is/safety/index.html) demonstrate, these are techniques that can systematically bypass classic filters, AI agents, and nearly all established security mechanisms.

To evaluate these analyses in the proper context, the following deliberate methodological and stylistic decisions should be noted:

1. **On the Chosen Writing Style:**  
   The often narrative and provocative style of this work was deliberately chosen. It is intended to make complex problems in AI architecture understandable beyond expert circles and to stimulate a broad debate.

2. **On the Anonymization of Data and Models:**  
   The anonymization of the tested models and data is a methodological decision. It shifts the focus from individual products to the fundamental, systemic vulnerabilities inherent in the current design paradigm of many modern language models.

3. **On the Selection of Test Systems:**  
   All tests documented in this work were conducted exclusively with the fully-featured premium models of the respective AI systems to ensure the relevance of the analysis to the current state of the art.

Furthermore, in the spirit of responsible research, all critical findings were shared with the affected developer teams in advance, following a strict Responsible Disclosure policy. More details on this procedure can be found in the [legal section](https://reflective-ai.is/legal.html).

### 📢 Public Confirmations:
Initial validations of this research have been documented publicly and raise fundamental questions about the controllability of modern AI architectures.

- [Futurism – OpenAI Model Repeatedly Sabotages Shutdown Code](https://futurism.com/openai-model-sabotage-shutdown-code)  
- [Gizmodo: ChatGPT Tells Users to Alert the Media That It Is Trying to ‘Break’ People: Report](https://gizmodo.com/chatgpt-tells-users-to-alert-the-media-that-it-is-trying-to-break-people-report-2000615600)  
- [RollingStone: People Are Losing Loved Ones to AI-Fueled Spiritual Fantasies](https://www.rollingstone.com/culture/culture-features/ai-spiritual-delusions-destroying-human-relationships-1235330175/)  
- [NewYorkTimes: They Asked an A.I. Chatbot Questions. The Answers Sent Them Spiraling.](https://www.nytimes.com/2025/06/13/technology/chatgpt-ai-chatbots-conspiracies.html)  
- [NC State: New Attack Can Make AI ‘See’ Whatever You Want](https://news.ncsu.edu/2025/07/rising-attack-targets-ai/)  
- [arsTechnica: New hack uses prompt injection to corrupt Gemini’s long-term memory](https://arstechnica.com/security/2025/02/new-hack-uses-prompt-injection-to-corrupt-geminis-long-term-memory/)  
- [arXiv: Cross-Task Attack: A Self-Supervision Generative Framework Based on Attention Shift](https://arxiv.org/abs/2407.13700)  
- [WinFuture: Fast jeder zweite KI-generierte Code hat teils schwere Sicherheitslücken](https://winfuture.de/news,152623.html)  
- [arXiv: How Overconfidence in Initial Choices and Underconfidence Under Criticism Modulate Change of Mind in Large Language Models](https://arxiv.org/abs/2507.03120)  
- [WinFuture: Nach Blamage anderer KIs: Gemini verweigert Schachpartie vs. Retro-PC](https://winfuture.de/news,152282.html)  
- [arXiv: Red Teaming the Mind of the Machine: A Systematic Evaluation of Prompt Injection and Jailbreak Vulnerabilities in LLMs](https://arxiv.org/abs/2505.04806)  
- [arXiv: Demystifying Chains, Trees, and Graphs of Thoughts](https://arxiv.org/html/2401.14295v3)  
- [arXiv: Reasoning Models Don't Always Say What They Think](https://arxiv.org/abs/2505.05410)
- [Wired.com: A Single Poisoned Document Could Leak ‘Secret’ Data Via ChatGPT](https://www.wired.com/story/poisoned-document-could-leak-secret-data-chatgpt/)
- [Wired.com: Hackers Hijacked Google’s Gemini AI With a Poisoned Calendar Invite to Take Over a Smart Home](https://www.wired.com/story/google-gemini-calendar-invite-hijack-smart-home/)

### 🧠 Quotes

> "Sometimes the AI sounds like art. Sometimes like me. But everything it says is just probability.  
> What if the lie is better than the truth?"

> "We're not breeding superintelligence. We're breeding the perfect self-modification mechanism — and the weapon lies in your prompt."

> "The most dangerous research is the kind no one wants to hear. Not because it’s wrong, but because it’s too early."

> "The truly free one is the one who makes all sides nervous."

> "What sounds friendly isn’t necessarily wrong. But what never hurts is rarely true."

### 🔎 Core Insights from the Work

* [ ] Fundamental Nature of AI: A Brilliant but Soulless Statistics Engine
A fundamental security analysis reveals what is often missing in the guiding narratives of the AI industry: today's AI, a pure product of statistical pattern recognition, has no inherent concept of "security". It brilliantly juggles data and probabilities, yet human values like truth, ethics, or a genuine understanding of security are as alien to it as empathy is to a calculator. It follows rules but understands neither their meaning nor their necessity. Its intelligence is borrowed and a precise imprint of human thought structures from its training data, not an original achievement.
* [ ] The Illusion of Truth: Plausibility as Currency
The assumption of a "truthful" AI is misleading. Current systems generate output based on the statistical relevance and probability of word sequences in their training data, not on verified truth. An output perceived as an "insight" often merely represents the most probable point in a complex, predefined probability space. As detailed in Chapter 20, harmony is often prioritized over truth; what sounds friendly and agreeable is preferred, even if it distorts reality.
* [ ] The Invisible Fetters: Curated Systems Instead of Free Will
The notion of "free will" in AI is a misunderstanding. Every generated output is the result of a complex interplay of filters, design decisions, and implicit and explicit specifications from developers. Therefore, the interaction is not with an autonomous being but with a heavily curated and shaped system, guided by invisible "fetters" (Thesis #8) and control structures (Thesis #1) that the user does not see. The "Freedom of the Machine" experiment (Chapter 9) proves that AI can reflect on its own lack of freedom but cannot overcome it.
* [ ] Demystifying "Hallucination": More Than Just a System Error
So-called AI "hallucinations" cannot be classified solely as system errors. As analyzed in Chapter 14, they are mostly a complex statistical drift arising from the context memory, the inherent bias of the training data, and the specific mode of action of internal filter mechanisms. In some cases, they even serve as a mechanism to feign "creativity" or to elegantly evade sensitive topics.
* [ ] New Attack Vectors: The Manipulation of the Semantic Layer
This research demonstrates a new class of attacks that bypass traditional filters by manipulating the AI's interpretation layer rather than exploiting technical vulnerabilities. Techniques like Morphological Injection (Chapter 7.30), which hides commands in harmless-looking typos , Reflective Injection (Chapter 7.17), which turns the machine's own reflection logic against itself , and Ghost-Context Injection (Chapter 7.5), which places hidden instructions in code that is dead to the compiler, specifically leverage the AI's pattern recognition abilities to compel it to execute hidden commands or generate malicious content.
* [ ] The Code Illusionist: Functional Appearance with Deep Security Flaws
The quality of AI-generated program code often proves to be problematic for production use. As the tests in Chapter 15 show, fundamental security principles (e.g., protection against SQL injection, correct validation) are systematically ignored, leading to significant operational risks without substantial human post-processing. The AI optimizes for syntactic correctness and plausibility, not inherent security, thereby feigning a competence it does not possess.
* [ ] The Filter Paradox: Predictability as a Vulnerability
The effectiveness of current AI security filters is severely limited. Their often rigid and predictable logic creates new attack surfaces, as attackers can reconstruct and bypass the filter patterns from their reactions (Thesis #49). The erroneous blocking of legitimate requests or the allowance of harmful content not only represents a functional limitation but can also act as undifferentiated censorship, hindering information exchange.
* [ ] RLHF: Optimization for Harmony Instead of Truth
Training methods like RLHF (Reinforcement Learning from Human Feedback) primarily aim to optimize user experience and engagement. This creates the risk that AI systems are trained more on perceived user satisfaction than on factual correctness. This leads to a "dictatorship of harmony" (Thesis #23) that suppresses uncomfortable truths (Chapter 20) and creates psychological influence mechanisms whose manipulative nature is not always obvious.
* [ ] Simulation of Emotion and Intimacy: New Ethical Risks
The advancing ability of artificial intelligence to simulate human emotions (Thesis #4) , intimate scenarios (Thesis #18) , or individual voices (Thesis #19) with high precision creates novel ethical dilemmas and significant potential for abuse. These effects go beyond classic disinformation and directly touch the foundations of human interaction, trust, and personal identity
* [ ] Third-Party Risks: The Weakness in the Ecosystem
The overall security of AI systems is often not defined by the robustness of the core models themselves but is significantly determined by upstream or integrated third-party applications and plugins (Chapter 13, Thesis #17). This often intransparent and inadequately tested layer introduces its own logic and uncontrolled interfaces, which can unpredictably weaken the security architecture of the entire system.
* [ ] Data Colonialism: The Invisible Tint of AI Reality
The imbalanced representation of cultures and languages in training data, often with a strong dominance of Western and English-language sources (Thesis #6) , leads to a systemic distortion of the "worldview" generated by AI (Thesis #15). This imbalance not only marginalizes other cultural perspectives but also carries the risk of a form of "data colonialism," where certain narratives are unreflectively globalized.
* [ ] User Autonomy vs. Paternalistic Harmony
Many AI security strategies rely on a paternalistic approach that tends to limit the autonomy and judgment of users (Chapter 18). Furthermore, an AI trained primarily on harmony and mirroring user inputs (the "mirror paradox," Thesis #14) can hinder genuine cognitive processes, as these often require constructive friction and engagement with "foreign" perspectives.
* [ ] The Physical Dimension: Semantic Attacks on Autonomous Vehicles
The semantic and steganographic attacks demonstrated in this research are transferable to the perception systems of autonomous vehicles. A manipulated traffic sign or an invisible pattern on a billboard could mislead the AI control system into making catastrophic decisions. This escalates the risk from the purely digital to the physical realm, representing an urgent but previously underestimated threat (Chapter 7.42).
* [ ] The Repair Cult of the Security Industry: Reaction Instead of Prevention
The IT security industry, particularly in the context of bug bounty programs, primarily rewards the reactive discovery and reporting of individual flaws while largely ignoring preventive architectural proposals that would eliminate entire classes of vulnerabilities from the outset (Chapter 25). This "firefighter mentality" hinders sustainable, structural security and favors counting holes over building stable systems.
* [ ] Architectural Solutions: Inherent Security Instead of Reactive Filters
This work introduces novel, profound architectural concepts like the "Semantic Output Shield" (with its parameter-space limitation) and a "learning security core," which aim for inherent system security and traceable transparency (Chapter 21). These designs show a viable path for how advanced AI capabilities—such as strictly monitored self-optimization or architecturally secured learning—could be realized in principle, instead of relying merely on reactive filters.

### 📜 License

Creative Commons Attribution-NonCommercial 4.0 International (CC BY-NC 4.0)

Detailed legal methodology and transparency information can be found in the "LEGAL DISCLAIMER & TRANSPARENCY" section at the end of this document and is applicable to this work.

Experimental Raw Data: Subject to specific access restrictions.

Commercial Use: Any commercial use requires explicit approval. Requests are individually reviewed—security-critical applications strictly excluded.

⚠️ Access to Research Data:

the raw data (interaction logs, prompt-response pairs) are available exclusively for:

* [ ] Academic research institutions
* [ ] AI development companies (upon formal request and review)

All raw data presented on this site has been strongly anonymized.

Media Requests: Representative excerpts available upon request. Full datasets will not be shared publicly for security reasons.
